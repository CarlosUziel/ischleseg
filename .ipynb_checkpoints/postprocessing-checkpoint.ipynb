{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Imports\n",
    "import os\n",
    "import shutil\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "from scipy import ndimage\n",
    "from nilearn.image import resample_to_img, resample_img\n",
    "from nilearn.masking import compute_background_mask, compute_epi_mask\n",
    "from nilearn.plotting import plot_roi, plot_epi\n",
    "from scipy.spatial.distance import directed_hausdorff\n",
    "from nipype.algorithms.metrics import Distance\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from scipy import interp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set working directory\n",
    "os.chdir('/home/uziel/DISS')\n",
    "# Set root of models to be post-processed\n",
    "root = \"./milestones_4\"\n",
    "model_variant = 'DM_V0_R_transfer_[0-4]' # choose model variant. Eg. \"DM_V0_[0-4]\".\n",
    "tmp = model_variant.split('_')\n",
    "if len(tmp) == 3:\n",
    "    model_name = tmp[1]\n",
    "elif len(tmp) == 4:\n",
    "    model_name = tmp[1] + '_' + tmp[2]\n",
    "else:\n",
    "    model_name = tmp[1] + '_' + tmp[2] + '_' + tmp[3]\n",
    "trained_models = sorted(glob(os.path.join(root, model_variant)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_metrics(original_path, predicted_path, mask_path, th=None):\n",
    "    original_data = nib.load(original_path).get_data()\n",
    "    predicted_data = nib.load(predicted_path).get_data()\n",
    "    mask_data = nib.load(mask_path).get_data() # use mask to limit results to the brain\n",
    "    \n",
    "    # Threshold data if necessary\n",
    "    if th is not None:\n",
    "        predicted_data = (predicted_data > th).astype(int)\n",
    "    \n",
    "    metrics = {}\n",
    "    \n",
    "    # Real positive cases\n",
    "    metrics['P'] = float(np.sum((original_data == 1).astype(int) * mask_data))\n",
    "    \n",
    "    # Real negative cases\n",
    "    metrics['N'] = float(np.sum((original_data == 0).astype(int) * mask_data))\n",
    "    \n",
    "    # True positive\n",
    "    metrics['TP'] = float(np.sum((((predicted_data == 1).astype(int) +\n",
    "                                   (original_data == 1).astype(int)) * mask_data) == 2))\n",
    "    \n",
    "    # True negative\n",
    "    metrics['TN'] = float(np.sum((((predicted_data == 0).astype(int) +\n",
    "                                   (original_data == 0).astype(int)) * mask_data) == 2))\n",
    "    \n",
    "    # False positive (all 1's in predicted minus original 1's)\n",
    "    metrics['FP'] = float(np.sum((((predicted_data == 1).astype(int) -\n",
    "                                   (original_data == 1).astype(int)) * mask_data) == 1))\n",
    "    \n",
    "    # False negative\n",
    "    metrics['FN'] = float(np.sum((((predicted_data == 1).astype(int) -\n",
    "                                   (original_data == 1).astype(int)) * mask_data) == -1))\n",
    "\n",
    "    # True positive rate (Sensitivity, Recall)\n",
    "    metrics['TPR'] = metrics['TP'] / (metrics['TP'] + metrics['FN'])  \n",
    "    \n",
    "    # True negative rate (Specificity)\n",
    "    metrics['TNR'] = metrics['TN'] / (metrics['TN'] + metrics['FP'])\n",
    "    \n",
    "    # Positive predictive value (Precision)\n",
    "    metrics['PPV'] = metrics['TP'] / (metrics['TP'] + metrics['FP'])\n",
    "    \n",
    "    # Negative predictive value\n",
    "    metrics['NPV'] = metrics['TN'] / (metrics['TN'] + metrics['FN'])\n",
    "    \n",
    "    # False negative rate (Miss rate)\n",
    "    metrics['FNR'] = 1 -  metrics['TPR']\n",
    "    \n",
    "    # False positive rate (Fall-out)\n",
    "    metrics['FPR'] = 1 - metrics['TNR']\n",
    "    \n",
    "    # False discovery rate\n",
    "    metrics['FDR'] = 1 - metrics['PPV']\n",
    "    \n",
    "    # False omission rate\n",
    "    metrics['FOR'] = 1 - metrics['NPV']\n",
    "    \n",
    "    # Accuracy\n",
    "    metrics['ACC'] = (metrics['TP'] + metrics['TN']) / \\\n",
    "                                (metrics['TP'] + \n",
    "                                 metrics['TN'] + \n",
    "                                 metrics['FP'] + \n",
    "                                 metrics['FN'])\n",
    "    \n",
    "    # F1 Score (also known as DSC, Sørensen–Dice coefficient, ...)\n",
    "    #metrics['F1S'] = 2 * (metrics['PPV'] * metrics['TPR']) / \\\n",
    "    #                                (metrics['PPV'] + metrics['TPR'])\n",
    "    metrics['F1S'] = (2*metrics['TP']) / (2*metrics['TP'] + metrics['FP'] + metrics['FN'])\n",
    "    \n",
    "    # Matthews correlation coefficient\n",
    "    # The MCC can be more appropriate when negatives actually mean something,\n",
    "    # and can be more useful in other ways.\n",
    "    metrics['MCC'] = ((metrics['TP'] * metrics['TN']) - (metrics['FP'] * metrics['FN'])) / \\\n",
    "                        np.sqrt(\n",
    "                            (metrics['TP'] + metrics['FP']) *\n",
    "                            (metrics['TP'] + metrics['FN']) *\n",
    "                            (metrics['TN'] + metrics['FP']) *\n",
    "                            (metrics['TN'] + metrics['FN']))\n",
    "    \n",
    "    # Compute Hausdorff distance\n",
    "    D = Distance()\n",
    "    if th is not None:        \n",
    "        metrics['HD'] = D._eucl_max(nib.load(original_path),\n",
    "                                    nib.Nifti1Image(predicted_data, nib.load(predicted_path).affine))\n",
    "    else:\n",
    "        metrics['HD'] = D._eucl_max(nib.load(original_path), nib.load(predicted_path))\n",
    "    \n",
    "    # Compute Jaccard index\n",
    "    metrics['JI'] = metrics['TP'] / (metrics['FN'] + metrics['FP'] + metrics['TP'])\n",
    "    \n",
    "    # Informedness or Bookmaker informedness\n",
    "    metrics['BM'] = metrics['TPR'] + metrics['TNR'] - 1\n",
    "    \n",
    "    #Markedness\n",
    "    metrics['MK'] = metrics['PPV'] + metrics['NPV'] - 1\n",
    "    \n",
    "    return(metrics)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**POSTPROCESSING FOR TEST CASES**\n",
    "\n",
    "Upsample predicted labels and compute test metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################\n",
    "##### POSTPROCESSING FOR K-FOLD CROSS-VALIDATION MODELS (0) ######\n",
    "##################################################################\n",
    "\n",
    "root_data = './data/ISLES2017/training'\n",
    "root_data_processed = './data_processed/ISLES2017/training'\n",
    "results = {}\n",
    "\n",
    "for model in trained_models:\n",
    "    root_1 = os.path.join(model, 'output/predictions/testSession/predictions')\n",
    "    root_2 = os.path.dirname(root_1)\n",
    "    \n",
    "    # Load label predictions\n",
    "    preds = sorted(glob(os.path.join(root_1, '*Segm.nii.gz')))\n",
    "    # Load probability maps of background\n",
    "    pmap_0 = sorted(glob(os.path.join(root_1, '*ProbMapClass0.nii.gz')))\n",
    "    # Load probability maps of foreground\n",
    "    pmap_1 = sorted(glob(os.path.join(root_1, '*ProbMapClass1.nii.gz')))\n",
    "    \n",
    "    results[os.path.basename(model)] = []\n",
    "    \n",
    "    # resize its prediction for final result validation\n",
    "    for i in range(len(preds)):\n",
    "        # Find subject that contains the code in pred.\n",
    "        subject = sorted([y\n",
    "                          for x in os.walk(root_data)\n",
    "                          for y in glob(os.path.join(x[0], '*'))\n",
    "                          if os.path.basename(preds[i]).split('_')[-2].split('.')[-1] in y\n",
    "                         ])[0].split('/')[-2]\n",
    "\n",
    "        subject_channels = sorted([y\n",
    "                                   for x in os.walk(os.path.join(root_data, subject))\n",
    "                                   for y in glob(os.path.join(x[0], '*MR_*.nii'))\n",
    "                                   if '4DPWI' not in y\n",
    "                                  ])\n",
    "        \n",
    "        subject_label = sorted([y\n",
    "                                for x in os.walk(os.path.join(root_data, subject))\n",
    "                                for y in glob(os.path.join(x[0], '*OT*.nii'))\n",
    "                               ])[0]\n",
    "\n",
    "        subject_processed = sorted([y\n",
    "                                    for x in os.walk(root_data_processed)\n",
    "                                    for y in glob(os.path.join(x[0], '*'))\n",
    "                                    if os.path.basename(preds[i]).split('_')[-2].split('.')[-1] in y\n",
    "                                   ])[0].split('/')[-2]\n",
    "        \n",
    "        subject_mask = sorted([y\n",
    "                               for x in os.walk(os.path.join(root_data_processed, subject_processed))\n",
    "                               for y in glob(os.path.join(x[0], '*mask*'))\n",
    "                              ])[0]\n",
    "        \n",
    "        # Load ADC channel as reference\n",
    "        original_img = nib.load(subject_channels[0])\n",
    "\n",
    "        # Load predictions and prob maps\n",
    "        pred_img = nib.load(preds[i])\n",
    "        pmap_0_img = nib.load(pmap_0[i])\n",
    "        pmap_1_img = nib.load(pmap_1[i])\n",
    "        \n",
    "        # Upsample to original size\n",
    "        pred_img = resample_img(pred_img,\n",
    "                                original_img.affine,\n",
    "                                original_img.shape,\n",
    "                                interpolation='nearest')\n",
    "        \n",
    "        pmap_0_img = resample_img(pmap_0_img,\n",
    "                                  original_img.affine,\n",
    "                                  original_img.shape,\n",
    "                                  interpolation='continuous')\n",
    "        \n",
    "        pmap_1_img = resample_img(pmap_1_img,\n",
    "                                  original_img.affine,\n",
    "                                  original_img.shape,\n",
    "                                  interpolation='continuous')\n",
    "        \n",
    "        # Load subject mask\n",
    "        mask_img = nib.load(subject_mask)\n",
    "        \n",
    "        # Upsample to original size\n",
    "        mask_img = resample_img(mask_img,\n",
    "                                original_img.affine,\n",
    "                                original_img.shape,\n",
    "                                interpolation='nearest')\n",
    "        \n",
    "        # Save prediction\n",
    "        pred_path = os.path.join(root_2, \"_\".join(os.path.basename(preds[i]).split('_')[:-1]) + '.pred.nii')\n",
    "        pmap_0_path = os.path.join(root_2, \"_\".join(os.path.basename(pmap_0[i]).split('_')[:-1]) + '.pmap_0.nii')\n",
    "        pmap_1_path = os.path.join(root_2, \"_\".join(os.path.basename(pmap_1[i]).split('_')[:-1]) + '.pmap_1.nii')\n",
    "        mask_path = os.path.join(root_2, \"_\".join(os.path.basename(pmap_1[i]).split('_')[:-1]) + '.mask.nii')\n",
    "        \n",
    "        nib.save(pred_img, pred_path)\n",
    "        nib.save(pmap_0_img, pmap_0_path)\n",
    "        nib.save(pmap_1_img, pmap_1_path)\n",
    "        nib.save(mask_img, mask_path)\n",
    "        \n",
    "        # Compute metrics between original and predicted label\n",
    "        metrics = get_test_metrics(subject_label, pred_path, mask_path)\n",
    "        \n",
    "        results[os.path.basename(model)].append([subject,\n",
    "                                                 subject_channels,\n",
    "                                                 subject_label,\n",
    "                                                 pred_path,\n",
    "                                                 pmap_0_path,\n",
    "                                                 pmap_1_path,\n",
    "                                                 mask_path,\n",
    "                                                 metrics])\n",
    "        \n",
    "    # Save model results\n",
    "    with open(os.path.join(model, 'test_results.pkl'), 'wb') as output:\n",
    "        pickle.dump(results[os.path.basename(model)], output, pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "    # Compute mean and std of model subjects predictions' metrics\n",
    "    metrics = np.array(results[os.path.basename(model)])[:,7]\n",
    "    test_metrics = {}\n",
    "    test_metrics['mean'] = {k : np.mean([t[k] for t in metrics]) for k in metrics[0]}\n",
    "    test_metrics['std'] = {k : np.std([t[k] for t in metrics]) for k in metrics[0]}\n",
    "    \n",
    "    # Save each model's metrics\n",
    "    with open(os.path.join(model, 'test_metrics.pkl'), 'wb') as output:\n",
    "        pickle.dump(test_metrics, output, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# Save all models' results\n",
    "with open(os.path.join(root, model_name + '_test_results.pkl'), 'wb') as output:\n",
    "    pickle.dump(results, output, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load each model's metrics, compute mean and variance. This is the final result of an experiment, and determines its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = []\n",
    "for model in trained_models:\n",
    "    with open(os.path.join(model, 'test_metrics.pkl'), 'rb') as input:\n",
    "        metrics.append(pickle.load(input))\n",
    "\n",
    "metrics = np.array(metrics)\n",
    "test_metrics['mean'] = {k : np.mean([t['mean'][k] for t in metrics]) for k in metrics[0]['mean']}\n",
    "test_metrics['std'] = {k : np.std([t['std'][k] for t in metrics]) for k in metrics[0]['std']}\n",
    "\n",
    "# Save final experiment metrics\n",
    "with open(os.path.join(root, model_name + '_test_metrics.pkl'), 'wb') as output:\n",
    "    pickle.dump(test_metrics, output, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot original and predicted labels for test cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot original label and predicted label on top of original image\n",
    "for model in trained_models:\n",
    "    \n",
    "    plt.close('all')\n",
    "    fig = plt.figure(figsize=(16, len(results[os.path.basename(model)])*2))\n",
    "    i = 1\n",
    "\n",
    "    for subject, subject_channels, subject_label, pred_label, _, _, _, metrics in results[os.path.basename(model)]:\n",
    "        original_img = nib.load(subject_channels[0])\n",
    "        original_label_img = nib.load(subject_label)\n",
    "        predicted_label_img = nib.load(pred_label)\n",
    "        \n",
    "        ax = fig.add_subplot(len(results[os.path.basename(model)]), 2, i)\n",
    "        ax.set_title('Ground truth for subject: ' + subject)\n",
    "        temp = plot_roi(original_label_img, original_img, display_mode='z', cut_coords=4, figure=fig, axes=ax)\n",
    "        ax = fig.add_subplot(len(results[os.path.basename(model)]), 2, i+1)\n",
    "        ax.set_title('Prediction. DICE: %.2f, HD: %.2f, JI: %.2f, SEN: %.2f, SPE: %.2f.'\n",
    "                             % (metrics['F1S'], metrics['HD'], metrics['JI'], metrics['TPR'], metrics['TNR']))\n",
    "        plot_roi(predicted_label_img, original_img, display_mode='z', cut_coords=temp.cut_coords, figure=fig, axes=ax)\n",
    "        i += 2\n",
    "\n",
    "    plt.savefig(os.path.join(model, 'testSegResults_' + os.path.basename(model) + '.pdf'), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ROC CURVE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mean_fpr = {}\n",
    "model_mean_tpr = {}\n",
    "model_mean_th = {}\n",
    "model_mean_auc = {}\n",
    "for model in trained_models:\n",
    "    original_data = []\n",
    "    predicted_data = []\n",
    "\n",
    "    for _, _, subject_label, _, _, pmap_1_path, _, _ in results[os.path.basename(model)]:\n",
    "        original_data.append(nib.load(subject_label).get_data().ravel())\n",
    "        predicted_data.append(nib.load(pmap_1_path).get_data().ravel())\n",
    "\n",
    "    n = len(results[os.path.basename(model)]) # number of subjects\n",
    "    fpr = {}\n",
    "    tpr = {}\n",
    "    th = {}\n",
    "    for i in range(n):\n",
    "        fpr[i], tpr[i], th[i] = roc_curve(original_data[i], predicted_data[i], pos_label = 1)\n",
    "    \n",
    "    # http://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html\n",
    "    # First aggregate all false positive rates\n",
    "    all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n)]))\n",
    "\n",
    "    # Then interpolate all ROC curves at this points\n",
    "    mean_tpr = np.zeros_like(all_fpr)\n",
    "    mean_th = np.zeros_like(all_fpr)\n",
    "    for i in range(n):\n",
    "        mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
    "        mean_th += interp(all_fpr, fpr[i], th[i])\n",
    "\n",
    "    # Finally average it and compute AUC\n",
    "    mean_tpr /= n\n",
    "    mean_th /= n    \n",
    "    \n",
    "    model_mean_fpr[os.path.basename(model)] = all_fpr\n",
    "    model_mean_tpr[os.path.basename(model)] = mean_tpr\n",
    "    model_mean_th[os.path.basename(model)] = mean_th\n",
    "    model_mean_auc[os.path.basename(model)] = auc(all_fpr, mean_tpr)\n",
    "\n",
    "plt.figure()\n",
    "lw = 2\n",
    "for model in trained_models:\n",
    "    plt.plot(model_mean_fpr[os.path.basename(model)], model_mean_tpr[os.path.basename(model)], lw=lw,\n",
    "             label = '{0} (AUC = {1:0.2f})'\n",
    "             ''.format(os.path.basename(model), model_mean_auc[os.path.basename(model)]))\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=lw, label = 'Reference')\n",
    "plt.plot([0, 1], [1, 0], 'k:', lw=lw, label = 'EER')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC curve (' + model_variant + ')')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig(os.path.join(root, model_name + '_roc.pdf'), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Equal error rate (EER)**\n",
    "\n",
    "Compute optimal threshold for each subject to achieve minimum error rate, the intersection between FNR (1-TPR) and FPR (1-TNR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_test_metrics_eer = {}\n",
    "for model in trained_models:\n",
    "    all_test_metrics_eer[os.path.basename(model)] = []\n",
    "    for _, _, subject_label, _, _, pmap_1_path, mask_path, _ in results[os.path.basename(model)]:\n",
    "        original_data = nib.load(subject_label).get_data()\n",
    "        predicted_data = nib.load(pmap_1_path).get_data()\n",
    "\n",
    "        # Compute fpr and tpr for multiple thresholds\n",
    "        fpr, tpr, th = roc_curve(original_data.ravel(), predicted_data.ravel())\n",
    "\n",
    "        # intersection between EER line and TPR\n",
    "        eer_line = np.arange(1, 0, -1.0/len(tpr))\n",
    "        while len(eer_line) > len(tpr): # bug workaround\n",
    "            eer_line = eer_line[:-1]\n",
    "        idx = np.argwhere(np.diff(np.sign(tpr - eer_line)) != 0).reshape(-1)[0]\n",
    "        \n",
    "        # Get equal error rate and optimal threshold at intersection\n",
    "        eer = tpr[idx]\n",
    "        th_op = th[idx]\n",
    "\n",
    "        # Compute new metrics after new threshold\n",
    "        metrics = get_test_metrics(subject_label, pmap_1_path, mask_path, th_op)\n",
    "        all_test_metrics_eer[os.path.basename(model)].append(metrics)\n",
    "\n",
    "    metrics = np.array(all_test_metrics_eer[os.path.basename(model)])\n",
    "    test_metrics_eer = {}\n",
    "    test_metrics_eer['mean'] = {k : np.mean([t[k] for t in metrics]) for k in metrics[0]}\n",
    "    test_metrics_eer['std'] = {k : np.std([t[k] for t in metrics]) for k in metrics[0]}\n",
    "\n",
    "    with open(os.path.join(model, 'test_metrics_eer.pkl'), 'wb') as output:\n",
    "        pickle.dump(test_metrics_eer, output, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# Save all metrics after EER for future reference\n",
    "with open(os.path.join(root, model_name +  '_test_results_eer.pkl'), 'wb') as output:\n",
    "    pickle.dump(all_test_metrics_eer, output, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load each model's metrics, compute mean and variance. This is the final result of an experiment, and determines its performance (after applying EER)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = []\n",
    "for model in trained_models:\n",
    "    with open(os.path.join(model, 'test_metrics_eer.pkl'), 'rb') as input:\n",
    "        metrics.append(pickle.load(input))\n",
    "\n",
    "metrics = np.array(metrics)\n",
    "test_metrics_eer['mean'] = {k : np.mean([t['mean'][k] for t in metrics]) for k in metrics[0]['mean']}\n",
    "test_metrics_eer['std'] = {k : np.std([t['std'][k] for t in metrics]) for k in metrics[0]['std']}\n",
    "\n",
    "# Save final experiment metrics after eer\n",
    "with open(os.path.join(root, model_name + '_test_metrics_eer.pkl'), 'wb') as output:\n",
    "    pickle.dump(test_metrics_eer, output, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PROCESS TRAINING AND VALIDATION RESULTS**\n",
    "\n",
    "Plot and save training progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in trained_models:\n",
    "    # Plot and save training progress\n",
    "    os.system(\"python ischleseg/deepmedic/plotSaveTrainingProgress.py \" +\n",
    "              os.path.join(model, \"output/logs/trainSession.txt -d -m 20 -s\"))\n",
    "    # Move files to the corresponding model directory\n",
    "    os.system(\"mv trainingProgress.pdf \" + os.path.join(model, 'trainingProgress_' + os.path.basename(model) + '.pdf'))\n",
    "    os.system(\"mv trainingProgress.pkl \" + os.path.join(model, 'trainingProgress.pkl'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load training metrics and compute mean and variance between models (includes training and validation metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load \"measuredMetricsFromAllExperiments\"\n",
    "# 1st dimension: \"Validation\" (0), \"Training\" (1)\n",
    "# 2nd dimension: ? (0)\n",
    "# 3rd dimension: \"Mean Accuracy\" (0), \"Sensitivity\" (1), \"Specificity\" (2), \"DSC (samples)\" (3), \"DSC (full-segm)\" (4)\n",
    "\n",
    "metrics = {}\n",
    "for model in trained_models:\n",
    "    with open(os.path.join(model, 'trainingProgress.pkl'), 'rb') as input:\n",
    "        metrics[os.path.basename(model)] = np.array(pickle.load(input))\n",
    "        metrics[os.path.basename(model)][0,0,4] = np.array(metrics[os.path.basename(model)][0,0,4])\n",
    "        \n",
    "# Compute mean and variance of all models' variations metrics\n",
    "metrics_mean = {}\n",
    "metrics_var = {}\n",
    "metrics_values = np.array(metrics.values())\n",
    "metrics_names_0 = ['Validation', 'Training']\n",
    "metrics_names_1 = ['Mean Accuracy', 'Sensitivity', 'Specificity', 'DSC (Samples)', 'DSC (full-segm)']\n",
    "\n",
    "for i in range(len(metrics_names_0)):\n",
    "    metrics_mean[metrics_names_0[i]] = {}\n",
    "    metrics_var[metrics_names_0[i]] = {}\n",
    "    for j in range(len(metrics_names_1)):\n",
    "        if i == 1 and j == 4: # Skip DSC_full for training (is never calculated)\n",
    "            metrics_mean[metrics_names_0[i]][metrics_names_1[j]] = np.zeros(35*20)\n",
    "            metrics_var[metrics_names_0[i]][metrics_names_1[j]] = np.zeros(35*20)\n",
    "            continue \n",
    "        metrics_mean[metrics_names_0[i]][metrics_names_1[j]] = np.mean(metrics_values[:,i,0,j])\n",
    "        metrics_var[metrics_names_0[i]][metrics_names_1[j]] = np.var(metrics_values[:,i,0,j])\n",
    "\n",
    "train_val_metrics = {}\n",
    "train_val_metrics['mean'] = metrics_mean\n",
    "train_val_metrics['var'] = metrics_var\n",
    "# Save final experiment progress metrics\n",
    "with open(os.path.join(root, model_name + '_train_val_metrics.pkl'), 'wb') as output:\n",
    "    pickle.dump(train_val_metrics, output, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot mean training and validation progress metrics of all trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = []\n",
    "for model in trained_models:\n",
    "    with open(os.path.join(model, 'test_metrics.pkl'), 'rb') as input:\n",
    "        metrics.append(pickle.load(input))\n",
    "\n",
    "plt.close('all')\n",
    "rows, cols = [2, 5]\n",
    "fig = plt.figure(figsize=(cols*6, rows*4))\n",
    "for i in range(len(metrics_names_0)):\n",
    "    for j in range(len(metrics_names_1)):\n",
    "        ax = fig.add_subplot(rows, cols, i * cols + 1 + j)\n",
    "        if i == 0 and j == 4:\n",
    "            plt.plot(np.arange(0, 40, 5), metrics_mean[metrics_names_0[i]][metrics_names_1[j]], 'r')\n",
    "        else:            \n",
    "            plt.plot(np.arange(0, 35, 1/20.0), metrics_mean[metrics_names_0[i]][metrics_names_1[j]], 'r')\n",
    "        plt.xlim(0, 35)\n",
    "        plt.ylim(0, 1.0)\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel(metrics_names_0[i])\n",
    "        plt.title(metrics_names_1[j])\n",
    "        ax.yaxis.grid(True)\n",
    "\n",
    "# Save mean training and validation metrics of all trained models averaged\n",
    "plt.subplots_adjust(hspace=0.5)\n",
    "plt.savefig(os.path.join(root, model_name + '_meanTrainProgress.pdf'), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/uziel/DISS/milestones_4/V0_test_metrics.pkl', 'rb') as input:\n",
    "    test_metrics_V0 = pickle.load(input)\n",
    "with open('/home/uziel/DISS/milestones_4/V0_test_metrics_eer.pkl', 'rb') as input:\n",
    "    test_metrics_V0_eer = pickle.load(input)\n",
    "with open('/home/uziel/DISS/milestones_4/V0_transfer_test_metrics.pkl', 'rb') as input:\n",
    "    test_metrics_V0_transfer = pickle.load(input)\n",
    "with open('/home/uziel/DISS/milestones_4/V0_transfer_test_metrics_eer.pkl', 'rb') as input:\n",
    "    test_metrics_V0_transfer_eer = pickle.load(input)\n",
    "with open('/home/uziel/DISS/milestones_4/V1_test_metrics.pkl', 'rb') as input:\n",
    "    test_metrics_V1 = pickle.load(input)\n",
    "with open('/home/uziel/DISS/milestones_4/V1_test_metrics_eer.pkl', 'rb') as input:\n",
    "    test_metrics_V1_eer = pickle.load(input)\n",
    "with open('/home/uziel/DISS/milestones_4/V1_transfer_test_metrics.pkl', 'rb') as input:\n",
    "    test_metrics_V1_transfer = pickle.load(input)\n",
    "with open('/home/uziel/DISS/milestones_4/V1_transfer_test_metrics_eer.pkl', 'rb') as input:\n",
    "    test_metrics_V1_transfer_eer = pickle.load(input)\n",
    "with open('/home/uziel/DISS/milestones_4/V2_test_metrics.pkl', 'rb') as input:\n",
    "    test_metrics_V2 = pickle.load(input)\n",
    "with open('/home/uziel/DISS/milestones_4/V2_test_metrics_eer.pkl', 'rb') as input:\n",
    "    test_metrics_V2_eer = pickle.load(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/uziel/DISS/milestones_4/V0_R_test_metrics.pkl', 'rb') as input:\n",
    "    test_metrics_V0_R = pickle.load(input)\n",
    "with open('/home/uziel/DISS/milestones_4/V0_R_test_metrics_eer.pkl', 'rb') as input:\n",
    "    test_metrics_V0_R_eer = pickle.load(input)\n",
    "with open('/home/uziel/DISS/milestones_4/V0_R_transfer_test_metrics.pkl', 'rb') as input:\n",
    "    test_metrics_V0_R = pickle.load(input)\n",
    "with open('/home/uziel/DISS/milestones_4/V0_R_transfer_test_metrics_eer.pkl', 'rb') as input:\n",
    "    test_metrics_V0_R_eer = pickle.load(input)\n",
    "with open('/home/uziel/DISS/milestones_4/V1_R_test_metrics.pkl', 'rb') as input:\n",
    "    test_metrics_V0_R = pickle.load(input)\n",
    "with open('/home/uziel/DISS/milestones_4/V1_R_test_metrics_eer.pkl', 'rb') as input:\n",
    "    test_metrics_V0_R_eer = pickle.load(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
