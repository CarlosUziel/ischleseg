{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Imports\n",
    "import os\n",
    "import shutil\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "from scipy import ndimage\n",
    "from nilearn.image import resample_to_img, resample_img\n",
    "from nilearn.masking import compute_background_mask, compute_epi_mask\n",
    "from nilearn.plotting import plot_roi, plot_epi\n",
    "from scipy.spatial.distance import directed_hausdorff\n",
    "from nipype.algorithms.metrics import Distance\n",
    "from sklearn.metrics import roc_curve, auc, f1_score, precision_recall_curve, average_precision_score\n",
    "from scipy import interp\n",
    "from itertools import chain\n",
    "from scipy.ndimage.morphology import binary_dilation, binary_erosion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(original_path, predicted_path, mask_path, th=None):\n",
    "    original_data = nib.load(original_path).get_data()\n",
    "    predicted_data = nib.load(predicted_path).get_data()\n",
    "    mask_data = nib.load(mask_path).get_data() # use mask to limit results to the brain\n",
    "    \n",
    "    # Threshold data if necessary\n",
    "    if th is not None:\n",
    "        predicted_data = (predicted_data > th).astype(int)\n",
    "    \n",
    "    metrics = {}\n",
    "    \n",
    "    # Real positive cases\n",
    "    metrics['P'] = float(np.sum((original_data == 1).astype(int) * mask_data))\n",
    "\n",
    "    # Real negative cases\n",
    "    metrics['N'] = float(np.sum((original_data == 0).astype(int) * mask_data))\n",
    "\n",
    "    # True positive\n",
    "    metrics['TP'] = float(np.sum((((predicted_data == 1).astype(int) +\n",
    "                                   (original_data == 1).astype(int)) * mask_data) == 2))\n",
    "\n",
    "    # True negative\n",
    "    metrics['TN'] = float(np.sum((((predicted_data == 0).astype(int) +\n",
    "                                   (original_data == 0).astype(int)) * mask_data) == 2))\n",
    "\n",
    "    # False positive (all 1's in predicted minus original 1's)\n",
    "    metrics['FP'] = float(np.sum((((predicted_data == 1).astype(int) -\n",
    "                                   (original_data == 1).astype(int)) * mask_data) == 1))\n",
    "\n",
    "    # False negative\n",
    "    metrics['FN'] = float(np.sum((((predicted_data == 1).astype(int) -\n",
    "                                   (original_data == 1).astype(int)) * mask_data) == -1))\n",
    "\n",
    "    # True positive rate (Sensitivity, Recall)\n",
    "    metrics['TPR'] = metrics['TP'] / (metrics['TP'] + metrics['FN'])  \n",
    "\n",
    "    # True negative rate (Specificity)\n",
    "    metrics['TNR'] = metrics['TN'] / (metrics['TN'] + metrics['FP'])\n",
    "\n",
    "    # Positive predictive value (Precision)\n",
    "    metrics['PPV'] = metrics['TP'] / (metrics['TP'] + metrics['FP'])\n",
    "\n",
    "    # Negative predictive value\n",
    "    metrics['NPV'] = metrics['TN'] / (metrics['TN'] + metrics['FN'])\n",
    "\n",
    "    # False negative rate (Miss rate)\n",
    "    metrics['FNR'] = 1 -  metrics['TPR']\n",
    "\n",
    "    # False positive rate (Fall-out)\n",
    "    metrics['FPR'] = 1 - metrics['TNR']\n",
    "\n",
    "    # False discovery rate\n",
    "    metrics['FDR'] = 1 - metrics['PPV']\n",
    "\n",
    "    # False omission rate\n",
    "    metrics['FOR'] = 1 - metrics['NPV']\n",
    "\n",
    "    # Accuracy\n",
    "    metrics['ACC'] = (metrics['TP'] + metrics['TN']) / \\\n",
    "                                (metrics['TP'] + \n",
    "                                 metrics['TN'] + \n",
    "                                 metrics['FP'] + \n",
    "                                 metrics['FN'])\n",
    "\n",
    "    # F1 Score (also known as DSC, Sørensen–Dice coefficient, ...)\n",
    "    #metrics['F1S'] = 2 * (metrics['PPV'] * metrics['TPR']) / \\\n",
    "    #                                (metrics['PPV'] + metrics['TPR'])\n",
    "    metrics['F1S'] = (2*metrics['TP']) / (2*metrics['TP'] + metrics['FP'] + metrics['FN'])\n",
    "\n",
    "    # Matthews correlation coefficient\n",
    "    # The MCC can be more appropriate when negatives actually mean something,\n",
    "    # and can be more useful in other ways.\n",
    "    metrics['MCC'] = ((metrics['TP'] * metrics['TN']) - (metrics['FP'] * metrics['FN'])) / \\\n",
    "                        np.sqrt(\n",
    "                            (metrics['TP'] + metrics['FP']) *\n",
    "                            (metrics['TP'] + metrics['FN']) *\n",
    "                            (metrics['TN'] + metrics['FP']) *\n",
    "                            (metrics['TN'] + metrics['FN']))\n",
    "\n",
    "    # Compute Hausdorff distance\n",
    "    D = Distance()\n",
    "    if th is not None:\n",
    "        try:\n",
    "            metrics['HD'] = D._eucl_max(nib.load(original_path),\n",
    "                                        nib.Nifti1Image(predicted_data, nib.load(predicted_path).affine))\n",
    "        except:\n",
    "            metrics['HD'] = float('nan')\n",
    "    else:\n",
    "        metrics['HD'] = D._eucl_max(nib.load(original_path), nib.load(predicted_path))\n",
    "\n",
    "    # Compute Jaccard index\n",
    "    metrics['JI'] = metrics['TP'] / (metrics['FN'] + metrics['FP'] + metrics['TP'])\n",
    "\n",
    "    # Informedness or Bookmaker informedness\n",
    "    metrics['BM'] = metrics['TPR'] + metrics['TNR'] - 1\n",
    "\n",
    "    #Markedness\n",
    "    metrics['MK'] = metrics['PPV'] + metrics['NPV'] - 1\n",
    "\n",
    "    \n",
    "    return(metrics)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set working directory\n",
    "os.chdir('/home/uziel/DISS')\n",
    "# Set root of models to be post-processed\n",
    "root = \"./milestones_4\"\n",
    "model_variant = 'DM_V2_[0-4]' # choose model variant. Eg. \"DM_V0_[0-4]\".\n",
    "tmp = model_variant.split('_')\n",
    "if len(tmp) == 3:\n",
    "    model_name = tmp[1]\n",
    "elif len(tmp) == 4:\n",
    "    model_name = tmp[1] + '_' + tmp[2]\n",
    "else:\n",
    "    model_name = tmp[1] + '_' + tmp[2] + '_' + tmp[3]\n",
    "    \n",
    "# Load all trained models (k-folds) of model_variant\n",
    "trained_models = sorted(glob(os.path.join(root, model_variant)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**POSTPROCESSING** for test cases\n",
    "\n",
    "Upsample predicted labels and compute test metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################\n",
    "##### POSTPROCESSING FOR K-FOLD CROSS-VALIDATION MODELS (0) ######\n",
    "##################################################################\n",
    "\n",
    "root_data = './data/ISLES2017/training'\n",
    "root_data_processed = './data_processed/ISLES2017/training'\n",
    "test_results = {}\n",
    "\n",
    "for model in trained_models:\n",
    "    root_1 = os.path.join(model, 'output/predictions/testSession/predictions')\n",
    "    root_2 = os.path.dirname(root_1)\n",
    "    \n",
    "    # Load label predictions\n",
    "    preds = sorted(glob(os.path.join(root_1, '*Segm.nii.gz')))\n",
    "    # Load probability maps of background\n",
    "    pmap_0 = sorted(glob(os.path.join(root_1, '*ProbMapClass0.nii.gz')))\n",
    "    # Load probability maps of foreground\n",
    "    pmap_1 = sorted(glob(os.path.join(root_1, '*ProbMapClass1.nii.gz')))\n",
    "    \n",
    "    test_results[os.path.basename(model)] = []\n",
    "    \n",
    "    # resize its prediction for final result validation\n",
    "    for i in range(len(preds)):\n",
    "        # Find subject that contains the code in pred.\n",
    "        subject = sorted([y\n",
    "                          for x in os.walk(root_data)\n",
    "                          for y in glob(os.path.join(x[0], '*'))\n",
    "                          if os.path.basename(preds[i]).split('_')[-2].split('.')[-1] in y\n",
    "                         ])[0].split('/')[-2]\n",
    "\n",
    "        subject_channels = sorted([y\n",
    "                                   for x in os.walk(os.path.join(root_data, subject))\n",
    "                                   for y in glob(os.path.join(x[0], '*MR_*.nii'))\n",
    "                                   if '4DPWI' not in y\n",
    "                                  ])\n",
    "        \n",
    "        subject_label = sorted([y\n",
    "                                for x in os.walk(os.path.join(root_data, subject))\n",
    "                                for y in glob(os.path.join(x[0], '*OT*.nii'))\n",
    "                               ])[0]\n",
    "\n",
    "        subject_processed = sorted([y\n",
    "                                    for x in os.walk(root_data_processed)\n",
    "                                    for y in glob(os.path.join(x[0], '*'))\n",
    "                                    if os.path.basename(preds[i]).split('_')[-2].split('.')[-1] in y\n",
    "                                   ])[0].split('/')[-2]\n",
    "        \n",
    "        subject_mask = sorted([y\n",
    "                               for x in os.walk(os.path.join(root_data_processed, subject_processed))\n",
    "                               for y in glob(os.path.join(x[0], '*mask*'))\n",
    "                              ])[0]\n",
    "        \n",
    "        # Load ADC channel as reference\n",
    "        original_img = nib.load(subject_channels[0])\n",
    "\n",
    "        # Load predictions and prob maps\n",
    "        pred_img = nib.load(preds[i])\n",
    "        pmap_0_img = nib.load(pmap_0[i])\n",
    "        pmap_1_img = nib.load(pmap_1[i])\n",
    "        \n",
    "        # Upsample to original size\n",
    "        pred_img = resample_img(pred_img,\n",
    "                                original_img.affine,\n",
    "                                original_img.shape,\n",
    "                                interpolation='nearest')\n",
    "        \n",
    "        pmap_0_img = resample_img(pmap_0_img,\n",
    "                                  original_img.affine,\n",
    "                                  original_img.shape,\n",
    "                                  interpolation='continuous')\n",
    "        \n",
    "        pmap_1_img = resample_img(pmap_1_img,\n",
    "                                  original_img.affine,\n",
    "                                  original_img.shape,\n",
    "                                  interpolation='continuous')\n",
    "        \n",
    "        # Load subject mask\n",
    "        mask_img = nib.load(subject_mask)\n",
    "        \n",
    "        # Upsample to original size\n",
    "        mask_img = resample_img(mask_img,\n",
    "                                original_img.affine,\n",
    "                                original_img.shape,\n",
    "                                interpolation='nearest')\n",
    "        \n",
    "        # Save prediction\n",
    "        pred_path = os.path.join(root_2, \"_\".join(os.path.basename(preds[i]).split('_')[:-1]) + '.pred.nii')\n",
    "        pmap_0_path = os.path.join(root_2, \"_\".join(os.path.basename(pmap_0[i]).split('_')[:-1]) + '.pmap_0.nii')\n",
    "        pmap_1_path = os.path.join(root_2, \"_\".join(os.path.basename(pmap_1[i]).split('_')[:-1]) + '.pmap_1.nii')\n",
    "        mask_path = os.path.join(root_2, \"_\".join(os.path.basename(pmap_1[i]).split('_')[:-1]) + '.mask.nii')\n",
    "        \n",
    "        nib.save(pred_img, pred_path)\n",
    "        nib.save(pmap_0_img, pmap_0_path)\n",
    "        nib.save(pmap_1_img, pmap_1_path)\n",
    "        nib.save(mask_img, mask_path)\n",
    "        \n",
    "        # Compute metrics between original and predicted label\n",
    "        metrics = get_metrics(subject_label, pred_path, mask_path)\n",
    "        \n",
    "        test_results[os.path.basename(model)].append([subject,\n",
    "                                                 subject_channels,\n",
    "                                                 subject_label,\n",
    "                                                 pred_path,\n",
    "                                                 pmap_0_path,\n",
    "                                                 pmap_1_path,\n",
    "                                                 mask_path,\n",
    "                                                 metrics])\n",
    "        \n",
    "    # Save model results\n",
    "    with open(os.path.join(model, 'test_results.pkl'), 'wb') as output:\n",
    "        pickle.dump(test_results[os.path.basename(model)], output, pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "    # Compute mean and std of model subjects predictions' metrics\n",
    "    metrics = np.array(test_results[os.path.basename(model)])[:,7]\n",
    "    test_metrics = {}\n",
    "    test_metrics['mean'] = {k : np.mean([t[k] for t in metrics]) for k in metrics[0]}\n",
    "    test_metrics['std'] = {k : np.std([t[k] for t in metrics]) for k in metrics[0]}\n",
    "    \n",
    "    # Save each model's metrics\n",
    "    with open(os.path.join(model, 'test_metrics.pkl'), 'wb') as output:\n",
    "        pickle.dump(test_metrics, output, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# Save all models' results\n",
    "with open(os.path.join(root, model_name + '_test_results.pkl'), 'wb') as output:\n",
    "    pickle.dump(test_results, output, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load each model's metrics, compute mean and std. This is the final result of an experiment, and determines its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = []\n",
    "for model in trained_models:\n",
    "    with open(os.path.join(model, 'test_metrics.pkl'), 'rb') as input:\n",
    "        metrics.append(pickle.load(input))\n",
    "\n",
    "metrics = np.array(metrics)\n",
    "test_metrics['mean'] = {k : np.mean([t['mean'][k] for t in metrics]) for k in metrics[0]['mean']}\n",
    "test_metrics['std'] = {k : np.std([t['std'][k] for t in metrics]) for k in metrics[0]['std']}\n",
    "\n",
    "# Save final experiment metrics\n",
    "with open(os.path.join(root, model_name + '_test_metrics.pkl'), 'wb') as output:\n",
    "    pickle.dump(test_metrics, output, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot original and predicted labels for test cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot original label and predicted label on top of original image\n",
    "for model in trained_models:\n",
    "    \n",
    "    plt.close('all')\n",
    "    fig = plt.figure(figsize=(16, len(test_results[os.path.basename(model)])*2))\n",
    "    i = 1\n",
    "\n",
    "    for subject, subject_channels, subject_label, pred_label, _, _, _, metrics in test_results[os.path.basename(model)]:\n",
    "        original_img = nib.load(subject_channels[0])\n",
    "        original_label_img = nib.load(subject_label)\n",
    "        predicted_label_img = nib.load(pred_label)\n",
    "        \n",
    "        ax = fig.add_subplot(len(test_results[os.path.basename(model)]), 2, i)\n",
    "        ax.set_title('Ground truth for subject: ' + subject)\n",
    "        temp = plot_roi(original_label_img, original_img, display_mode='z', cut_coords=4, figure=fig, axes=ax)\n",
    "        ax = fig.add_subplot(len(test_results[os.path.basename(model)]), 2, i+1)\n",
    "        ax.set_title('Prediction. DICE: %.2f, HD: %.2f, JI: %.2f, SEN: %.2f, SPE: %.2f.'\n",
    "                             % (metrics['F1S'], metrics['HD'], metrics['JI'], metrics['TPR'], metrics['TNR']))\n",
    "        plot_roi(predicted_label_img, original_img, display_mode='z', cut_coords=temp.cut_coords, figure=fig, axes=ax)\n",
    "        i += 2\n",
    "\n",
    "    plt.savefig(os.path.join(model, 'testSegResults_' + os.path.basename(model) + '.pdf'), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ROC CURVE** for test cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mean_fpr = {}\n",
    "model_mean_tpr = {}\n",
    "model_mean_auc = {}\n",
    "for model in trained_models:\n",
    "    original_data = []\n",
    "    predicted_data = []\n",
    "    for _, _, subject_label, _, _, pmap_1_path, _, _ in test_results[os.path.basename(model)]:\n",
    "        original_data.append(nib.load(subject_label).get_data().ravel())\n",
    "        predicted_data.append(nib.load(pmap_1_path).get_data().ravel())\n",
    "\n",
    "    # Join all subjects to perform micro-average\n",
    "    y_true = list(chain.from_iterable(original_data))\n",
    "    y_pred = list(chain.from_iterable(predicted_data))\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_pred)\n",
    "    \n",
    "    model_mean_fpr[os.path.basename(model)] = fpr\n",
    "    model_mean_tpr[os.path.basename(model)] = tpr\n",
    "    model_mean_auc[os.path.basename(model)] = auc(fpr, tpr)\n",
    "    \n",
    "plt.figure()\n",
    "lw = 2\n",
    "tprs = []\n",
    "aucs = []\n",
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "for model in trained_models:\n",
    "    tprs.append(interp(mean_fpr, model_mean_fpr[os.path.basename(model)], model_mean_tpr[os.path.basename(model)]))\n",
    "    aucs.append(model_mean_auc[os.path.basename(model)])\n",
    "    plt.plot(model_mean_fpr[os.path.basename(model)], model_mean_tpr[os.path.basename(model)], lw=1, alpha=0.3,\n",
    "             label = 'Fold {0} (AUC = {1:0.2f})'\n",
    "             ''.format(os.path.basename(model).split('_')[-1], model_mean_auc[os.path.basename(model)]))\n",
    "\n",
    "\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',\n",
    "         label='Luck', alpha=.8)\n",
    "plt.plot([0, 1], [1, 0], 'k:', lw=lw, label = 'EER')\n",
    "\n",
    "mean_tpr = np.mean(tprs, axis=0)\n",
    "mean_tpr[-1] = 1.0\n",
    "mean_auc = auc(mean_fpr, mean_tpr)\n",
    "std_auc = np.std(aucs)\n",
    "plt.plot(mean_fpr, mean_tpr, color='b',\n",
    "         label=r'Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc, std_auc),\n",
    "         lw=2, alpha=.8)\n",
    "\n",
    "std_tpr = np.std(tprs, axis=0)\n",
    "tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
    "tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
    "plt.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2,\n",
    "                 label=r'$\\pm$ 1 std. dev.')\n",
    "\n",
    "plt.xlim([-0.05, 1.05])\n",
    "plt.ylim([-0.05, 1.05])\n",
    "\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic curve (' + model_name + ')')\n",
    "plt.legend(loc=(0.2, -0.85))\n",
    "plt.savefig(os.path.join(root, model_name + '_test_roc.pdf'), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PRECISION-RECALL CURVE** for test cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mean_precision = {}\n",
    "model_mean_recall = {}\n",
    "model_mean_ap = {}\n",
    "for model in trained_models:\n",
    "    original_data = []\n",
    "    predicted_data = []\n",
    "    for _, _, subject_label, _, _, pmap_1_path, _, _ in test_results[os.path.basename(model)]:\n",
    "        original_data.append(nib.load(subject_label).get_data().ravel())\n",
    "        predicted_data.append(nib.load(pmap_1_path).get_data().ravel())\n",
    "\n",
    "    # Join all subjects to perform micro-average\n",
    "    y_true = list(chain.from_iterable(original_data))\n",
    "    y_pred = list(chain.from_iterable(predicted_data))\n",
    "    precision, recall, _ = precision_recall_curve(y_true, y_pred)\n",
    "    average_precision = average_precision_score(y_true, y_pred)\n",
    "    \n",
    "    model_mean_precision[os.path.basename(model)] = precision\n",
    "    model_mean_recall[os.path.basename(model)] = recall\n",
    "    model_mean_ap[os.path.basename(model)] = average_precision\n",
    "\n",
    "plt.figure(figsize=(7, 8))\n",
    "\n",
    "f_scores = np.linspace(0.2, 0.8, num=4)\n",
    "lines = []\n",
    "labels = []\n",
    "for f_score in f_scores:\n",
    "    x = np.linspace(0.01, 1)\n",
    "    y = f_score * x / (2 * x - f_score)\n",
    "    l, = plt.plot(x[y >= 0], y[y >= 0], color='gray', alpha=0.2)\n",
    "    plt.annotate('F1S={0:0.1f}'.format(f_score), xy=(0.85, y[45] + 0.02))\n",
    "    \n",
    "lines.append(l)\n",
    "labels.append('iso-F1S curves')\n",
    "lw = 2\n",
    "for model in trained_models:\n",
    "    l, = plt.plot(model_mean_recall[os.path.basename(model)],\n",
    "                  model_mean_precision[os.path.basename(model)],\n",
    "                  lw=lw)\n",
    "    lines.append(l)\n",
    "    labels.append('Fold {0} (AP = {1:0.2f})'\n",
    "                  ''.format(os.path.basename(model).split('_')[-1],\n",
    "                            model_mean_ap[os.path.basename(model)]))\n",
    "\n",
    "fig = plt.gcf()\n",
    "fig.subplots_adjust(bottom=0.25)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.xlabel('True Positive Rate (Recall)')\n",
    "plt.ylabel('Positive Predictive Value (Precision)')\n",
    "plt.title('Precision-Recall curve (' + model_name + ')')\n",
    "plt.legend(lines, labels, loc=(0.3, -.5), prop=dict(size=14))\n",
    "plt.savefig(os.path.join(root, model_name + '_test_pr.pdf'), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**POSTPROCESSING** for validation cases\n",
    "\n",
    "Upsample predicted labels and compute validation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################\n",
    "##### POSTPROCESSING FOR K-FOLD CROSS-VALIDATION MODELS (0) ######\n",
    "##################################################################\n",
    "\n",
    "root_data = './data/ISLES2017/training'\n",
    "root_data_processed = './data_processed/ISLES2017/training'\n",
    "val_results = {}\n",
    "\n",
    "for model in trained_models:\n",
    "    root_1 = os.path.join(model, 'output/predictions/trainSession/predictions')\n",
    "    root_2 = os.path.dirname(root_1)\n",
    "    \n",
    "    # Load label predictions\n",
    "    preds = sorted(glob(os.path.join(root_1, '*Segm.nii.gz')))\n",
    "    # Load probability maps of background\n",
    "    pmap_0 = sorted(glob(os.path.join(root_1, '*ProbMapClass0.nii.gz')))\n",
    "    # Load probability maps of foreground\n",
    "    pmap_1 = sorted(glob(os.path.join(root_1, '*ProbMapClass1.nii.gz')))\n",
    "    \n",
    "    val_results[os.path.basename(model)] = []\n",
    "    \n",
    "    # resize its prediction for final result validation\n",
    "    for i in range(len(preds)):\n",
    "        # Find subject that contains the code in pred.\n",
    "        subject = sorted([y\n",
    "                          for x in os.walk(root_data)\n",
    "                          for y in glob(os.path.join(x[0], '*'))\n",
    "                          if os.path.basename(preds[i]).split('_')[-2].split('.')[-1] in y\n",
    "                         ])[0].split('/')[-2]\n",
    "\n",
    "        subject_channels = sorted([y\n",
    "                                   for x in os.walk(os.path.join(root_data, subject))\n",
    "                                   for y in glob(os.path.join(x[0], '*MR_*.nii'))\n",
    "                                   if '4DPWI' not in y\n",
    "                                  ])\n",
    "        \n",
    "        subject_label = sorted([y\n",
    "                                for x in os.walk(os.path.join(root_data, subject))\n",
    "                                for y in glob(os.path.join(x[0], '*OT*.nii'))\n",
    "                               ])[0]\n",
    "\n",
    "        subject_processed = sorted([y\n",
    "                                    for x in os.walk(root_data_processed)\n",
    "                                    for y in glob(os.path.join(x[0], '*'))\n",
    "                                    if os.path.basename(preds[i]).split('_')[-2].split('.')[-1] in y\n",
    "                                   ])[0].split('/')[-2]\n",
    "        \n",
    "        subject_mask = sorted([y\n",
    "                               for x in os.walk(os.path.join(root_data_processed, subject_processed))\n",
    "                               for y in glob(os.path.join(x[0], '*mask*'))\n",
    "                              ])[0]\n",
    "        \n",
    "        # Load ADC channel as reference\n",
    "        original_img = nib.load(subject_channels[0])\n",
    "\n",
    "        # Load predictions and prob maps\n",
    "        pred_img = nib.load(preds[i])\n",
    "        pmap_0_img = nib.load(pmap_0[i])\n",
    "        pmap_1_img = nib.load(pmap_1[i])\n",
    "        \n",
    "        # Upsample to original size\n",
    "        pred_img = resample_img(pred_img,\n",
    "                                original_img.affine,\n",
    "                                original_img.shape,\n",
    "                                interpolation='nearest')\n",
    "        \n",
    "        pmap_0_img = resample_img(pmap_0_img,\n",
    "                                  original_img.affine,\n",
    "                                  original_img.shape,\n",
    "                                  interpolation='continuous')\n",
    "        \n",
    "        pmap_1_img = resample_img(pmap_1_img,\n",
    "                                  original_img.affine,\n",
    "                                  original_img.shape,\n",
    "                                  interpolation='continuous')\n",
    "        \n",
    "        # Load subject mask\n",
    "        mask_img = nib.load(subject_mask)\n",
    "        \n",
    "        # Upsample to original size\n",
    "        mask_img = resample_img(mask_img,\n",
    "                                original_img.affine,\n",
    "                                original_img.shape,\n",
    "                                interpolation='nearest')\n",
    "        \n",
    "        # Save prediction\n",
    "        pred_path = os.path.join(root_2, \"_\".join(os.path.basename(preds[i]).split('_')[:-1]) + '.pred.nii')\n",
    "        pmap_0_path = os.path.join(root_2, \"_\".join(os.path.basename(pmap_0[i]).split('_')[:-1]) + '.pmap_0.nii')\n",
    "        pmap_1_path = os.path.join(root_2, \"_\".join(os.path.basename(pmap_1[i]).split('_')[:-1]) + '.pmap_1.nii')\n",
    "        mask_path = os.path.join(root_2, \"_\".join(os.path.basename(pmap_1[i]).split('_')[:-1]) + '.mask.nii')\n",
    "        \n",
    "        nib.save(pred_img, pred_path)\n",
    "        nib.save(pmap_0_img, pmap_0_path)\n",
    "        nib.save(pmap_1_img, pmap_1_path)\n",
    "        nib.save(mask_img, mask_path)\n",
    "        \n",
    "        # Compute metrics between original and predicted label\n",
    "        metrics = get_metrics(subject_label, pred_path, mask_path)\n",
    "        \n",
    "        val_results[os.path.basename(model)].append([subject,\n",
    "                                                 subject_channels,\n",
    "                                                 subject_label,\n",
    "                                                 pred_path,\n",
    "                                                 pmap_0_path,\n",
    "                                                 pmap_1_path,\n",
    "                                                 mask_path,\n",
    "                                                 metrics])\n",
    "        \n",
    "    # Save model results\n",
    "    with open(os.path.join(model, 'val_results.pkl'), 'wb') as output:\n",
    "        pickle.dump(val_results[os.path.basename(model)], output, pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "    # Compute mean and std of model subjects predictions' metrics\n",
    "    metrics = np.array(val_results[os.path.basename(model)])[:,7]\n",
    "    val_metrics = {}\n",
    "    val_metrics['mean'] = {k : np.mean([t[k] for t in metrics]) for k in metrics[0]}\n",
    "    val_metrics['std'] = {k : np.std([t[k] for t in metrics]) for k in metrics[0]}\n",
    "    \n",
    "    # Save each model's metrics\n",
    "    with open(os.path.join(model, 'val_metrics.pkl'), 'wb') as output:\n",
    "        pickle.dump(val_metrics, output, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# Save all models' results\n",
    "with open(os.path.join(root, model_name + '_val_results.pkl'), 'wb') as output:\n",
    "    pickle.dump(val_results, output, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load each model's metrics, compute mean and std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = []\n",
    "for model in trained_models:\n",
    "    with open(os.path.join(model, 'val_metrics.pkl'), 'rb') as input:\n",
    "        metrics.append(pickle.load(input))\n",
    "\n",
    "metrics = np.array(metrics)\n",
    "val_metrics['mean'] = {k : np.mean([t['mean'][k] for t in metrics]) for k in metrics[0]['mean']}\n",
    "val_metrics['std'] = {k : np.std([t['std'][k] for t in metrics]) for k in metrics[0]['std']}\n",
    "\n",
    "# Save final experiment metrics\n",
    "with open(os.path.join(root, model_name + '_val_metrics.pkl'), 'wb') as output:\n",
    "    pickle.dump(val_metrics, output, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot original and predicted labels for validation cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot original label and predicted label on top of original image\n",
    "for model in trained_models:\n",
    "    \n",
    "    plt.close('all')\n",
    "    fig = plt.figure(figsize=(16, len(val_results[os.path.basename(model)])*2))\n",
    "    i = 1\n",
    "\n",
    "    for subject, subject_channels, subject_label, pred_label, _, _, _, metrics in val_results[os.path.basename(model)]:\n",
    "        original_img = nib.load(subject_channels[0])\n",
    "        original_label_img = nib.load(subject_label)\n",
    "        predicted_label_img = nib.load(pred_label)\n",
    "        \n",
    "        ax = fig.add_subplot(len(val_results[os.path.basename(model)]), 2, i)\n",
    "        ax.set_title('Ground truth for subject: ' + subject)\n",
    "        temp = plot_roi(original_label_img, original_img, display_mode='z', cut_coords=4, figure=fig, axes=ax)\n",
    "        ax = fig.add_subplot(len(val_results[os.path.basename(model)]), 2, i+1)\n",
    "        ax.set_title('Prediction. DICE: %.2f, HD: %.2f, JI: %.2f, SEN: %.2f, SPE: %.2f.'\n",
    "                             % (metrics['F1S'], metrics['HD'], metrics['JI'], metrics['TPR'], metrics['TNR']))\n",
    "        plot_roi(predicted_label_img, original_img, display_mode='z', cut_coords=temp.cut_coords, figure=fig, axes=ax)\n",
    "        i += 2\n",
    "\n",
    "    plt.savefig(os.path.join(model, 'valSegResults_' + os.path.basename(model) + '.pdf'), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ROC CURVE** for validation cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mean_fpr = {}\n",
    "model_mean_tpr = {}\n",
    "model_mean_auc = {}\n",
    "for model in trained_models:\n",
    "    original_data = []\n",
    "    predicted_data = []\n",
    "    for _, _, subject_label, _, _, pmap_1_path, _, _ in val_results[os.path.basename(model)]:\n",
    "        original_data.append(nib.load(subject_label).get_data().ravel())\n",
    "        predicted_data.append(nib.load(pmap_1_path).get_data().ravel())\n",
    "\n",
    "    # Join all subjects to perform micro-average\n",
    "    y_true = list(chain.from_iterable(original_data))\n",
    "    y_pred = list(chain.from_iterable(predicted_data))\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_pred)\n",
    "    \n",
    "    model_mean_fpr[os.path.basename(model)] = fpr\n",
    "    model_mean_tpr[os.path.basename(model)] = tpr\n",
    "    model_mean_auc[os.path.basename(model)] = auc(fpr, tpr)\n",
    "    \n",
    "plt.figure()\n",
    "lw = 2\n",
    "tprs = []\n",
    "aucs = []\n",
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "for model in trained_models:\n",
    "    tprs.append(interp(mean_fpr, model_mean_fpr[os.path.basename(model)], model_mean_tpr[os.path.basename(model)]))\n",
    "    aucs.append(model_mean_auc[os.path.basename(model)])\n",
    "    plt.plot(model_mean_fpr[os.path.basename(model)], model_mean_tpr[os.path.basename(model)], lw=1, alpha=0.3,\n",
    "             label = 'Fold {0} (AUC = {1:0.2f})'\n",
    "             ''.format(os.path.basename(model).split('_')[-1], model_mean_auc[os.path.basename(model)]))\n",
    "\n",
    "\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',\n",
    "         label='Luck', alpha=.8)\n",
    "plt.plot([0, 1], [1, 0], 'k:', lw=lw, label = 'EER')\n",
    "\n",
    "mean_tpr = np.mean(tprs, axis=0)\n",
    "mean_tpr[-1] = 1.0\n",
    "mean_auc = auc(mean_fpr, mean_tpr)\n",
    "std_auc = np.std(aucs)\n",
    "plt.plot(mean_fpr, mean_tpr, color='b',\n",
    "         label=r'Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc, std_auc),\n",
    "         lw=2, alpha=.8)\n",
    "\n",
    "std_tpr = np.std(tprs, axis=0)\n",
    "tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
    "tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
    "plt.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2,\n",
    "                 label=r'$\\pm$ 1 std. dev.')\n",
    "\n",
    "plt.xlim([-0.05, 1.05])\n",
    "plt.ylim([-0.05, 1.05])\n",
    "\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic curve (' + model_name + ')')\n",
    "plt.legend(loc=(0.2, -0.85))\n",
    "plt.savefig(os.path.join(root, model_name + '_val_roc.pdf'), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PRECISION-RECALL CURVE** for validation cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mean_precision = {}\n",
    "model_mean_recall = {}\n",
    "model_mean_ap = {}\n",
    "for model in trained_models:\n",
    "    original_data = []\n",
    "    predicted_data = []\n",
    "    for _, _, subject_label, _, _, pmap_1_path, _, _ in val_results[os.path.basename(model)]:\n",
    "        original_data.append(nib.load(subject_label).get_data().ravel())\n",
    "        predicted_data.append(nib.load(pmap_1_path).get_data().ravel())\n",
    "\n",
    "    # Join all subjects to perform micro-average\n",
    "    y_true = list(chain.from_iterable(original_data))\n",
    "    y_pred = list(chain.from_iterable(predicted_data))\n",
    "    precision, recall, _ = precision_recall_curve(y_true, y_pred)\n",
    "    average_precision = average_precision_score(y_true, y_pred)\n",
    "    \n",
    "    model_mean_precision[os.path.basename(model)] = precision\n",
    "    model_mean_recall[os.path.basename(model)] = recall\n",
    "    model_mean_ap[os.path.basename(model)] = average_precision\n",
    "\n",
    "plt.figure(figsize=(7, 8))\n",
    "\n",
    "f_scores = np.linspace(0.2, 0.8, num=4)\n",
    "lines = []\n",
    "labels = []\n",
    "for f_score in f_scores:\n",
    "    x = np.linspace(0.01, 1)\n",
    "    y = f_score * x / (2 * x - f_score)\n",
    "    l, = plt.plot(x[y >= 0], y[y >= 0], color='gray', alpha=0.2)\n",
    "    plt.annotate('F1S={0:0.1f}'.format(f_score), xy=(0.85, y[45] + 0.02))\n",
    "    \n",
    "lines.append(l)\n",
    "labels.append('iso-F1S curves')\n",
    "lw = 2\n",
    "for model in trained_models:\n",
    "    l, = plt.plot(model_mean_recall[os.path.basename(model)],\n",
    "                  model_mean_precision[os.path.basename(model)],\n",
    "                  lw=lw)\n",
    "    lines.append(l)\n",
    "    labels.append('Fold {0} (AP = {1:0.2f})'\n",
    "                  ''.format(os.path.basename(model).split('_')[-1],\n",
    "                            model_mean_ap[os.path.basename(model)]))\n",
    "\n",
    "fig = plt.gcf()\n",
    "fig.subplots_adjust(bottom=0.25)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.xlabel('True Positive Rate (Recall)')\n",
    "plt.ylabel('Positive Predictive Value (Precision)')\n",
    "plt.title('Precision-Recall curve (' + model_name + ')')\n",
    "plt.legend(lines, labels, loc=(0.3, -.5), prop=dict(size=14))\n",
    "plt.savefig(os.path.join(root, model_name + '_val_pr.pdf'), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**THRESHOLD TUNING V0**\n",
    "\n",
    "Use precision-recall curve on validation cases to maximize F1S on test cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_test_metrics_tht_v0 = {}\n",
    "for model in trained_models:\n",
    "    # Compute best average threshold for validation cases\n",
    "    original_data = []\n",
    "    predicted_data = []\n",
    "    for _, _, subject_label, _, _, pmap_1_path, mask_path, _ in val_results[os.path.basename(model)]:\n",
    "        original_data.append(nib.load(subject_label).get_data().ravel())\n",
    "        predicted_data.append(nib.load(pmap_1_path).get_data().ravel())\n",
    "\n",
    "    # Join all subjects to perform micro-average\n",
    "    y_true = list(chain.from_iterable(original_data))\n",
    "    y_pred = list(chain.from_iterable(predicted_data))\n",
    "    precision, recall, thresholds = precision_recall_curve(y_true, y_pred)\n",
    "    # Optimal threshold is where precision * recall is maximum\n",
    "    tmp = precision * recall        \n",
    "    # The closest point is the furthest from bottom-left corner\n",
    "    idx = np.argwhere(tmp == np.max(tmp))\n",
    "    th_op = thresholds[idx]\n",
    "    \n",
    "    all_test_metrics_tht_v0[os.path.basename(model)] = []\n",
    "    # Recompute test metrics with optimal threshold from validation cases\n",
    "    for _, _, subject_label, _, _, pmap_1_path, mask_path, _ in test_results[os.path.basename(model)]:\n",
    "        original_data = nib.load(subject_label).get_data()\n",
    "        predicted_data = nib.load(pmap_1_path).get_data()\n",
    "        \n",
    "        # Compute new metrics after new threshold\n",
    "        metrics = get_metrics(subject_label, pmap_1_path, mask_path, th_op)\n",
    "        metrics['TH_OP'] = th_op\n",
    "        all_test_metrics_tht_v0[os.path.basename(model)].append(metrics)\n",
    "        \n",
    "    metrics = np.array(all_test_metrics_tht_v0[os.path.basename(model)])\n",
    "    test_metrics_tht_v0 = {}\n",
    "    test_metrics_tht_v0['mean'] = {k : np.nanmean([t[k] for t in metrics]) for k in metrics[0]}\n",
    "    test_metrics_tht_v0['std'] = {k : np.nanstd([t[k] for t in metrics]) for k in metrics[0]}\n",
    "\n",
    "    with open(os.path.join(model, 'test_metrics_tht_v0.pkl'), 'wb') as output:\n",
    "        pickle.dump(test_metrics_tht_v0, output, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# Save all metrics after threshold tuning v0 for future reference\n",
    "with open(os.path.join(root, model_name +  '_test_results_tht_v0.pkl'), 'wb') as output:\n",
    "    pickle.dump(all_test_metrics_tht_v0, output, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load each model's metrics after **threshold tuning v0**, compute mean and std."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = []\n",
    "for model in trained_models:\n",
    "    with open(os.path.join(model, 'test_metrics_tht_v0.pkl'), 'rb') as input:\n",
    "        metrics.append(pickle.load(input))\n",
    "\n",
    "metrics = np.array(metrics)\n",
    "test_metrics_tht_v0['mean'] = {k : np.nanmean([t['mean'][k] for t in metrics]) for k in metrics[0]['mean']}\n",
    "test_metrics_tht_v0['std'] = {k : np.nanstd([t['std'][k] for t in metrics]) for k in metrics[0]['std']}\n",
    "\n",
    "# Save final experiment metrics after tht_v0\n",
    "with open(os.path.join(root, model_name + '_test_metrics_tht_v0.pkl'), 'wb') as output:\n",
    "    pickle.dump(test_metrics_tht_v0, output, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**THRESHOLD TUNING V1**\n",
    "\n",
    "Use ROC curve on validation cases to maximize BM on test cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_test_metrics_tht_v1 = {}\n",
    "for model in trained_models:\n",
    "    # Compute best average threshold for validation cases\n",
    "    original_data = []\n",
    "    predicted_data = []\n",
    "    for _, _, subject_label, _, _, pmap_1_path, mask_path, _ in val_results[os.path.basename(model)]:\n",
    "        original_data.append(nib.load(subject_label).get_data().ravel())\n",
    "        predicted_data.append(nib.load(pmap_1_path).get_data().ravel())\n",
    "\n",
    "    # Join all subjects to perform micro-average\n",
    "    y_true = list(chain.from_iterable(original_data))\n",
    "    y_pred = list(chain.from_iterable(predicted_data))\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_pred)\n",
    "    # Optimal threshold is where tpr - fpr is maximum\n",
    "    d = tpr - fpr\n",
    "    idx = np.argwhere(d == np.max(d))\n",
    "    th_op = thresholds[idx]\n",
    "    \n",
    "    all_test_metrics_tht_v1[os.path.basename(model)] = []\n",
    "    # Recompute test metrics with optimal threshold from validation cases\n",
    "    for _, _, subject_label, _, _, pmap_1_path, mask_path, _ in test_results[os.path.basename(model)]:\n",
    "        original_data = nib.load(subject_label).get_data()\n",
    "        predicted_data = nib.load(pmap_1_path).get_data()\n",
    "        \n",
    "        # Compute new metrics after new threshold\n",
    "        metrics = get_metrics(subject_label, pmap_1_path, mask_path, th_op)\n",
    "        metrics['TH_OP'] = th_op\n",
    "        all_test_metrics_tht_v1[os.path.basename(model)].append(metrics)\n",
    "        \n",
    "    metrics = np.array(all_test_metrics_tht_v1[os.path.basename(model)])\n",
    "    test_metrics_tht_v1 = {}\n",
    "    test_metrics_tht_v1['mean'] = {k : np.nanmean([t[k] for t in metrics]) for k in metrics[0]}\n",
    "    test_metrics_tht_v1['std'] = {k : np.nanstd([t[k] for t in metrics]) for k in metrics[0]}\n",
    "\n",
    "    with open(os.path.join(model, 'test_metrics_tht_v1.pkl'), 'wb') as output:\n",
    "        pickle.dump(test_metrics_tht_v1, output, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# Save all metrics after threshold tuining for future reference\n",
    "with open(os.path.join(root, model_name +  '_test_results_tht_v1.pkl'), 'wb') as output:\n",
    "    pickle.dump(all_test_metrics_tht_v1, output, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load each model's metrics after **threshold tuning v1**, compute mean and std."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = []\n",
    "for model in trained_models:\n",
    "    with open(os.path.join(model, 'test_metrics_tht_v1.pkl'), 'rb') as input:\n",
    "        metrics.append(pickle.load(input))\n",
    "\n",
    "metrics = np.array(metrics)\n",
    "test_metrics_tht_v1['mean'] = {k : np.nanmean([t['mean'][k] for t in metrics]) for k in metrics[0]['mean']}\n",
    "test_metrics_tht_v1['std'] = {k : np.nanstd([t['std'][k] for t in metrics]) for k in metrics[0]['std']}\n",
    "\n",
    "# Save final experiment metrics after tht_v1\n",
    "with open(os.path.join(root, model_name + '_test_metrics_tht_v1.pkl'), 'wb') as output:\n",
    "    pickle.dump(test_metrics_tht_v1, output, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FILL HOLES**\n",
    "\n",
    "Dilate and erode to fill holes in the predicted label (predicted segmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_test_metrics_de = {}\n",
    "for model in trained_models:\n",
    "    all_test_metrics_de[os.path.basename(model)] = []\n",
    "    with open(os.path.join(model, 'test_metrics_tht_v0.pkl'), 'rb') as input:\n",
    "        metrics = pickle.load(input)\n",
    "        th = metrics['mean']['TH_OP']\n",
    "    for _, _, subject_label, _, _, pmap_1_path, mask_path, _ in test_results[os.path.basename(model)]:\n",
    "        original_data = nib.load(subject_label).get_data()\n",
    "        predicted_data = nib.load(pmap_1_path).get_data()\n",
    "        \n",
    "        # Define struct\n",
    "        struct = ndimage.generate_binary_structure(3, 3)\n",
    "        # Dilate image\n",
    "        img = binary_dilation(predicted_data > th, struct, iterations=6)\n",
    "        # Erode image\n",
    "        img = binary_erosion(img, struct, iterations=6)\n",
    "        # Save image temporarly\n",
    "        temp_path = 'temp.nii.gz'\n",
    "        nib.save(nib.Nifti1Image(img.astype(int), nib.load(subject_label).affine), temp_path)\n",
    "        # Recompute metrics\n",
    "        metrics = get_metrics(subject_label, temp_path, mask_path, 0)\n",
    "        metrics['TH_OP'] = th\n",
    "        all_test_metrics_de[os.path.basename(model)].append(metrics)\n",
    "        \n",
    "    metrics = np.array(all_test_metrics_de[os.path.basename(model)])\n",
    "    test_metrics_de = {}\n",
    "    test_metrics_de['mean'] = {k : np.nanmean([t[k] for t in metrics]) for k in metrics[0]}\n",
    "    test_metrics_de['std'] = {k : np.nanstd([t[k] for t in metrics]) for k in metrics[0]}\n",
    "\n",
    "    with open(os.path.join(model, 'test_metrics_de.pkl'), 'wb') as output:\n",
    "        pickle.dump(test_metrics_de, output, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# Save all metrics after dilation-erosion for future reference\n",
    "with open(os.path.join(root, model_name +  '_all_test_metrics_de.pkl'), 'wb') as output:\n",
    "    pickle.dump(all_test_metrics_de, output, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = []\n",
    "for model in trained_models:\n",
    "    with open(os.path.join(model, 'test_metrics_de.pkl'), 'rb') as input:\n",
    "        metrics.append(pickle.load(input))\n",
    "\n",
    "metrics = np.array(metrics)\n",
    "test_metrics_de['mean'] = {k : np.nanmean([t['mean'][k] for t in metrics]) for k in metrics[0]['mean']}\n",
    "test_metrics_de['std'] = {k : np.nanstd([t['std'][k] for t in metrics]) for k in metrics[0]['std']}\n",
    "\n",
    "# Save final experiment metrics after tht_v1\n",
    "with open(os.path.join(root, model_name + '_test_metrics_de.pkl'), 'wb') as output:\n",
    "    pickle.dump(test_metrics_de, output, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PROCESS TRAINING AND VALIDATION RESULTS**\n",
    "\n",
    "Plot and save training progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in trained_models:\n",
    "    # Plot and save training progress\n",
    "    os.system(\"python ischleseg/deepmedic/plotSaveTrainingProgress.py \" +\n",
    "              os.path.join(model, \"output/logs/trainSession.txt -d -m 20 -s\"))\n",
    "    # Move files to the corresponding model directory\n",
    "    os.system(\"mv trainingProgress.pdf \" + os.path.join(model, 'trainingProgress_' + os.path.basename(model) + '.pdf'))\n",
    "    os.system(\"mv trainingProgress.pkl \" + os.path.join(model, 'trainingProgress.pkl'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load training metrics and compute mean and variance between models (includes training and validation metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load \"measuredMetricsFromAllExperiments\"\n",
    "# 1st dimension: \"Validation\" (0), \"Training\" (1)\n",
    "# 2nd dimension: ? (0)\n",
    "# 3rd dimension: \"Mean Accuracy\" (0), \"Sensitivity\" (1), \"Specificity\" (2), \"DSC (samples)\" (3), \"DSC (full-segm)\" (4)\n",
    "\n",
    "metrics = {}\n",
    "for model in trained_models:\n",
    "    with open(os.path.join(model, 'trainingProgress.pkl'), 'rb') as input:\n",
    "        metrics[os.path.basename(model)] = np.array(pickle.load(input))\n",
    "        metrics[os.path.basename(model)][0,0,4] = np.array(metrics[os.path.basename(model)][0,0,4])\n",
    "        \n",
    "# Compute mean and variance of all models' variations metrics\n",
    "metrics_mean = {}\n",
    "metrics_std = {}\n",
    "metrics_values = np.array(metrics.values())\n",
    "metrics_names_0 = ['Validation', 'Training']\n",
    "metrics_names_1 = ['Mean Accuracy', 'Sensitivity', 'Specificity', 'DSC (Samples)', 'DSC (full-segm)']\n",
    "\n",
    "for i in range(len(metrics_names_0)):\n",
    "    metrics_mean[metrics_names_0[i]] = {}\n",
    "    metrics_std[metrics_names_0[i]] = {}\n",
    "    for j in range(len(metrics_names_1)):\n",
    "        if i == 1 and j == 4: # Skip DSC_full for training (is never calculated)\n",
    "            metrics_mean[metrics_names_0[i]][metrics_names_1[j]] = np.zeros(35*20)\n",
    "            metrics_std[metrics_names_0[i]][metrics_names_1[j]] = np.zeros(35*20)\n",
    "            continue \n",
    "        metrics_mean[metrics_names_0[i]][metrics_names_1[j]] = np.mean(metrics_values[:,i,0,j])\n",
    "        metrics_std[metrics_names_0[i]][metrics_names_1[j]] = np.std(metrics_values[:,i,0,j])\n",
    "\n",
    "train_val_metrics = {}\n",
    "train_val_metrics['mean'] = metrics_mean\n",
    "train_val_metrics['std'] = metrics_std\n",
    "# Save final experiment progress metrics\n",
    "with open(os.path.join(root, model_name + '_train_val_metrics.pkl'), 'wb') as output:\n",
    "    pickle.dump(train_val_metrics, output, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot mean training and validation progress metrics of all trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = []\n",
    "for model in trained_models:\n",
    "    with open(os.path.join(model, 'test_metrics.pkl'), 'rb') as input:\n",
    "        metrics.append(pickle.load(input))\n",
    "\n",
    "plt.close('all')\n",
    "rows, cols = [2, 5]\n",
    "fig = plt.figure(figsize=(cols*6, rows*4))\n",
    "for i in range(len(metrics_names_0)):\n",
    "    for j in range(len(metrics_names_1)):\n",
    "        ax = fig.add_subplot(rows, cols, i * cols + 1 + j)\n",
    "        upper = np.minimum(metrics_mean[metrics_names_0[i]][metrics_names_1[j]] +\n",
    "                           metrics_std[metrics_names_0[i]][metrics_names_1[j]], 1)\n",
    "        lower = np.maximum(metrics_mean[metrics_names_0[i]][metrics_names_1[j]] -\n",
    "                           metrics_std[metrics_names_0[i]][metrics_names_1[j]], 0)\n",
    "        if i == 0 and j == 4:\n",
    "            x = np.arange(0, 40, 5)\n",
    "        else:\n",
    "            x = np.arange(0, 35, 1/20.0)\n",
    "        \n",
    "        plt.plot(x, metrics_mean[metrics_names_0[i]][metrics_names_1[j]], 'r')        \n",
    "        plt.fill_between(x, lower, upper,\n",
    "                         color='grey', alpha=.2, label=r'$\\pm$ 1 std. dev.')\n",
    "        plt.xlim(0, 35)\n",
    "        plt.ylim(0, 1.0)\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel(metrics_names_0[i])\n",
    "        plt.title(metrics_names_1[j])\n",
    "        plt.legend(loc=(0.3, -0.4))\n",
    "        ax.yaxis.grid(True)\n",
    "\n",
    "# Save mean training and validation metrics of all trained models averaged\n",
    "plt.subplots_adjust(hspace=0.7)\n",
    "plt.savefig(os.path.join(root, model_name + '_meanTrainValProgress.pdf'), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# V0 files\n",
    "with open('/home/uziel/DISS/milestones_4/V0_test_metrics.pkl', 'rb') as input:\n",
    "    test_metrics_V0 = pickle.load(input)\n",
    "with open('/home/uziel/DISS/milestones_4/V0_test_metrics_tht_v0.pkl', 'rb') as input:\n",
    "    test_metrics_V0_tht_v0 = pickle.load(input)\n",
    "with open('/home/uziel/DISS/milestones_4/V0_test_metrics_tht_v1.pkl', 'rb') as input:\n",
    "    test_metrics_V0_tht_v1 = pickle.load(input)\n",
    "with open('/home/uziel/DISS/milestones_4/V0_test_metrics_de.pkl', 'rb') as input:\n",
    "    test_metrics_V0_de = pickle.load(input)\n",
    "# V0_transfer files\n",
    "with open('/home/uziel/DISS/milestones_4/V0_transfer_test_metrics.pkl', 'rb') as input:\n",
    "    test_metrics_V0_transfer = pickle.load(input)\n",
    "with open('/home/uziel/DISS/milestones_4/V0_transfer_test_metrics_tht_v0.pkl', 'rb') as input:\n",
    "    test_metrics_V0_transfer_tht_v0 = pickle.load(input)\n",
    "with open('/home/uziel/DISS/milestones_4/V0_transfer_test_metrics_tht_v1.pkl', 'rb') as input:\n",
    "    test_metrics_V0_transfer_tht_v1 = pickle.load(input)\n",
    "with open('/home/uziel/DISS/milestones_4/V0_transfer_test_metrics_de.pkl', 'rb') as input:\n",
    "    test_metrics_V0_transfer_de = pickle.load(input)\n",
    "# V1 files\n",
    "with open('/home/uziel/DISS/milestones_4/V1_test_metrics.pkl', 'rb') as input:\n",
    "    test_metrics_V1 = pickle.load(input)\n",
    "with open('/home/uziel/DISS/milestones_4/V1_test_metrics_tht_v0.pkl', 'rb') as input:\n",
    "    test_metrics_V1_tht_v0 = pickle.load(input)\n",
    "with open('/home/uziel/DISS/milestones_4/V1_test_metrics_tht_v1.pkl', 'rb') as input:\n",
    "    test_metrics_V1_tht_v1 = pickle.load(input)\n",
    "with open('/home/uziel/DISS/milestones_4/V1_test_metrics_de.pkl', 'rb') as input:\n",
    "    test_metrics_V1_de = pickle.load(input)\n",
    "# V1_transfer files\n",
    "with open('/home/uziel/DISS/milestones_4/V1_transfer_test_metrics.pkl', 'rb') as input:\n",
    "    test_metrics_V1_transfer = pickle.load(input)\n",
    "with open('/home/uziel/DISS/milestones_4/V1_transfer_test_metrics_tht_v0.pkl', 'rb') as input:\n",
    "    test_metrics_V1_transfer_tht_v0 = pickle.load(input)\n",
    "with open('/home/uziel/DISS/milestones_4/V1_transfer_test_metrics_tht_v1.pkl', 'rb') as input:\n",
    "    test_metrics_V1_transfer_tht_v1 = pickle.load(input)\n",
    "with open('/home/uziel/DISS/milestones_4/V1_transfer_test_metrics_de.pkl', 'rb') as input:\n",
    "    test_metrics_V1_transfer_de = pickle.load(input)\n",
    "# V2 files\n",
    "with open('/home/uziel/DISS/milestones_4/V2_test_metrics.pkl', 'rb') as input:\n",
    "    test_metrics_V2 = pickle.load(input)\n",
    "with open('/home/uziel/DISS/milestones_4/V2_test_metrics_tht_v0.pkl', 'rb') as input:\n",
    "    test_metrics_V2_tht_v0 = pickle.load(input)\n",
    "with open('/home/uziel/DISS/milestones_4/V2_test_metrics_tht_v1.pkl', 'rb') as input:\n",
    "    test_metrics_V2_tht_v1 = pickle.load(input)\n",
    "with open('/home/uziel/DISS/milestones_4/V2_test_metrics_de.pkl', 'rb') as input:\n",
    "    test_metrics_V2_de = pickle.load(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# V0_R files\n",
    "with open('/home/uziel/DISS/milestones_4/V0_R_test_metrics.pkl', 'rb') as input:\n",
    "    test_metrics_V0_R = pickle.load(input)\n",
    "with open('/home/uziel/DISS/milestones_4/V0_R_test_metrics_tht_v0.pkl', 'rb') as input:\n",
    "    test_metrics_V0_R_tht_v0 = pickle.load(input)\n",
    "with open('/home/uziel/DISS/milestones_4/V0_R_test_metrics_tht_v1.pkl', 'rb') as input:\n",
    "    test_metrics_V0_R_tht_v1 = pickle.load(input)\n",
    "# V0_R_transfer files\n",
    "with open('/home/uziel/DISS/milestones_4/V0_R_transfer_test_metrics.pkl', 'rb') as input:\n",
    "    test_metrics_V0_R_transfer = pickle.load(input)\n",
    "with open('/home/uziel/DISS/milestones_4/V0_R_transfer_test_metrics_tht_v0.pkl', 'rb') as input:\n",
    "    test_metrics_V0_R_transfer_tht_v0 = pickle.load(input)\n",
    "with open('/home/uziel/DISS/milestones_4/V0_R_transfer_test_metrics_tht_v1.pkl', 'rb') as input:\n",
    "    test_metrics_V0_R_transfer_tht_v1 = pickle.load(input)\n",
    "# V1_R files\n",
    "with open('/home/uziel/DISS/milestones_4/V1_R_test_metrics.pkl', 'rb') as input:\n",
    "    test_metrics_V1_R = pickle.load(input)\n",
    "with open('/home/uziel/DISS/milestones_4/V1_R_test_metrics_tht_v0.pkl', 'rb') as input:\n",
    "    test_metrics_V1_R_tht_v0 = pickle.load(input)\n",
    "with open('/home/uziel/DISS/milestones_4/V1_R_test_metrics_tht_v1.pkl', 'rb') as input:\n",
    "    test_metrics_V1_R_tht_v1 = pickle.load(input)\n",
    "# V1_R_transfer files\n",
    "with open('/home/uziel/DISS/milestones_4/V1_R_transfer_test_metrics.pkl', 'rb') as input:\n",
    "    test_metrics_V1_R_transfer = pickle.load(input)\n",
    "with open('/home/uziel/DISS/milestones_4/V1_R_transfer_test_metrics_tht_v0.pkl', 'rb') as input:\n",
    "    test_metrics_V1_R_transfer_tht_v0 = pickle.load(input)\n",
    "with open('/home/uziel/DISS/milestones_4/V1_R_transfer_test_metrics_tht_v1.pkl', 'rb') as input:\n",
    "    test_metrics_V1_R_transfer_tht_v1 = pickle.load(input)\n",
    "# V2_R files\n",
    "with open('/home/uziel/DISS/milestones_4/V2_R_test_metrics.pkl', 'rb') as input:\n",
    "    test_metrics_V2_R = pickle.load(input)\n",
    "with open('/home/uziel/DISS/milestones_4/V2_R_test_metrics_tht_v0.pkl', 'rb') as input:\n",
    "    test_metrics_V2_R_tht_v0 = pickle.load(input)\n",
    "with open('/home/uziel/DISS/milestones_4/V2_R_test_metrics_tht_v1.pkl', 'rb') as input:\n",
    "    test_metrics_V2_R_tht_v1 = pickle.load(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
