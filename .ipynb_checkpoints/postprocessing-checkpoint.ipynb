{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Imports\n",
    "import os\n",
    "import shutil\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "from scipy import ndimage\n",
    "from nilearn.image import resample_to_img, resample_img\n",
    "from nilearn.masking import compute_background_mask, compute_epi_mask\n",
    "from nilearn.plotting import plot_roi, plot_epi\n",
    "from scipy.spatial.distance import directed_hausdorff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_metrics(original_label, predicted_label):\n",
    "    original_data = nib.load(original_label).get_data()\n",
    "    predicted_data = nib.load(predicted_label).get_data()\n",
    "    \n",
    "    metrics = {}\n",
    "    \n",
    "    # True positive\n",
    "    metrics['TP'] = np.sum(original_data == 1)\n",
    "    \n",
    "    # True negative\n",
    "    metrics['TN'] = np.sum(original_data == 0)\n",
    "    \n",
    "    # False positive (all 1's in predicted minus original 1's)\n",
    "    metrics['FP'] = np.sum(((predicted_data == 1) - (original_data == 1)) > 0)\n",
    "    \n",
    "    # False negative\n",
    "    metrics['FN'] = np.sum(((predicted_data == 1) - (original_data == 1)) < 0)\n",
    "    \n",
    "    # True positive rate (Sensitivity, Recall)\n",
    "    metrics['TPR'] = metrics['TP'] / (metrics['TP'] + metrics['FN'])  \n",
    "    \n",
    "    # True negative rate (Specificity)\n",
    "    metrics['TNR'] = metrics['TN'] / (metrics['TN'] + metrics['FP'])\n",
    "    \n",
    "    # Positive predictive value (Precision)\n",
    "    metrics['PPV'] = metrics['TP'] / (metrics['TP'] + metrics['FP'])\n",
    "    \n",
    "    # Negative predictive value\n",
    "    metrics['NPV'] = metrics['TN'] / (metrics['TN'] + metrics['FN'])\n",
    "    \n",
    "    # False negative rate (Miss rate)\n",
    "    metrics['FNR'] = 1 -  metrics['TPR']\n",
    "    \n",
    "    # False positive rate (Fall-out)\n",
    "    metrics['FPR'] = 1 - metrics['TNR']\n",
    "    \n",
    "    # False discovery rate\n",
    "    metrics['FDR'] = 1 - metrics['PPV']\n",
    "    \n",
    "    # False omission rate\n",
    "    metrics['FOR'] = 1 - metrics['NPV']\n",
    "    \n",
    "    # Accuracy\n",
    "    metrics['ACC'] = (metrics['TP'] + metrics['TN']) / \\\n",
    "                                (metrics['TP'] + \n",
    "                                 metrics['TN'] + \n",
    "                                 metrics['FP'] + \n",
    "                                 metrics['FN'])\n",
    "    \n",
    "    # F1 Score (also known as DSC, Sørensen–Dice coefficient, ...)\n",
    "    metrics['F1S'] = 2 * (metrics['PPV'] * metrics['TPR']) / \\\n",
    "                                    (metrics['PPV'] + metrics['TPR'])\n",
    "    \n",
    "    # Matthews correlaton coefficient\n",
    "    # The MCC can be more appropriate when negatives actually mean something,\n",
    "    # and can be more useful in other ways.\n",
    "    metrics['MCC'] = metrics['TP'] * metrics['TN'] - \\\n",
    "                                metrics['FP'] * metrics['FN'] / \\\n",
    "                                np.sqrt((metrics['TP'] + metrics['FP']) *\n",
    "                                       (metrics['TP'] + metrics['FN']) *\n",
    "                                       (metrics['TN'] + metrics['FP']) *\n",
    "                                       (metrics['TN'] + metrics['FN']))\n",
    "    \n",
    "    # Compute Hausdorff distance\n",
    "    metrics['HD'] = directed_hausdorff(original_data, predicted_data)[0]\n",
    "    \n",
    "    # Compute Jaccard coefficient\n",
    "    metrics['JC'] = metrics['TP'] / (metrics['FN'] + metrics['FP'] + metrics['TP'])\n",
    "    \n",
    "    return(metrics)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set working directory\n",
    "os.chdir('/home/uziel/DISS')\n",
    "# Set root of models to be post-processed\n",
    "root = \"./milestones_3\"\n",
    "model_variant = '*' # choose model variant. Eg. \"DM_V0_{0..4}\".\n",
    "trained_models = sorted(glob(os.path.join(root, model_variant)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**POSTPROCESSING FOR TEST CASES**\n",
    "\n",
    "Upsample predicted labels and compute test metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################\n",
    "##### POSTPROCESSING FOR K-FOLD CROSS-VALIDATION MODELS (0) ######\n",
    "##################################################################\n",
    "\n",
    "root_data = './data/ISLES2017/testing'\n",
    "results = {}\n",
    "\n",
    "for model in trained_models:\n",
    "    root = os.path.join(model, 'output/predictions/testSession/predictions')\n",
    "    root_2 = os.path.dirname(root)\n",
    "    \n",
    "    preds = sorted(glob(os.path.join(root, '*Segm.nii.gz')))\n",
    "\n",
    "    results[os.path.basename(model)] = []\n",
    "    \n",
    "    # resize its prediction for final result validation\n",
    "    for i in range(len(preds)):\n",
    "        # Find subject that contains the code in pred.\n",
    "        subject = sorted([y\n",
    "                          for x in os.walk(root_data)\n",
    "                          for y in glob(os.path.join(x[0], '*'))\n",
    "                          if os.path.basename(preds[i]).split('_')[-2].split('.')[-1] in y\n",
    "                         ])[0].split('/')[-2]\n",
    "\n",
    "        subject_channels = sorted([y\n",
    "                                   for x in os.walk(os.path.join(root_data, subject))\n",
    "                                   for y in glob(os.path.join(x[0], '*MR_*.nii'))\n",
    "                                   if '4DPWI' not in y\n",
    "                                  ])\n",
    "        \n",
    "        subject_label = sorted([y\n",
    "                                for x in os.walk(os.path.join(root_data, subject))\n",
    "                                for y in glob(os.path.join(x[0], '*OT*.nii'))\n",
    "                               ])[0]\n",
    "\n",
    "        # Load ADC channel as reference\n",
    "        original_img = nib.load(subject_channels[0])\n",
    "\n",
    "        # Load prediction\n",
    "        pred_img = nib.load(preds[i])\n",
    "\n",
    "        # Upsample to original size\n",
    "        pred_img = resample_img(pred_img,\n",
    "                                original_img.affine,\n",
    "                                original_img.shape,\n",
    "                                interpolation='nearest')\n",
    "        \n",
    "        # Save prediction\n",
    "        pred_label = os.path.join(root_2, os.path.basename(preds[i]).split('_')[-2] + '.nii')\n",
    "        nib.save(pred_img, pred_label)\n",
    "        \n",
    "        # Compute metrics between original and predicted label\n",
    "        metrics = get_test_metrics(subject_label, pred_label)\n",
    "        \n",
    "        results[os.path.basename(model)].append([subject, subject_channels, subject_label, pred_label, metrics])\n",
    "        \n",
    "    # Save results\n",
    "    with open(os.path.join(model, 'test_results.txt'), 'wb') as output:\n",
    "        pickle.dump(results, output, pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "    # Compute mean and variance of subject predictions' metrics\n",
    "    metrics = np.array(results.values())[:,4]\n",
    "    test_metrics = {}\n",
    "    test_metrics['mean'] = {k : np.mean(t[k] for t in metrics) for k in metrics[0]}\n",
    "    test_metrics['var'] = {k : np.var(t[k] for t in metrics) for k in metrics[0]}\n",
    "    \n",
    "    # Save each model's metrics\n",
    "    with open(os.path.join(model, 'test_metrics.txt'), 'wb') as output:\n",
    "        pickle.dump(test_metrics, output, pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load each model's metrics, compute mean and variance. This is the final result of an experiment, and determines its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = []\n",
    "for model in trained_models:\n",
    "    with open(os.path.join(model, 'test_metrics.txt'), 'rb') as input:\n",
    "        metrics.append(pickle.load(input))\n",
    "        \n",
    "test_metrics['mean'] = {k : np.mean(t[k] for t in metrics) for k in metrics[0]}\n",
    "test_metrics['var'] = {k : np.var(t[k] for t in metrics) for k in metrics[0]}\n",
    "\n",
    "# Save final experiment metrics\n",
    "with open(os.path.join(root, model_variant + '_test_metrics.txt'), 'wb') as output:\n",
    "    pickle.dump(test_metrics, output, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot original and predicted labels for test cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot original label and predicted label on top of original image\n",
    "for model in trained_models:\n",
    "    \n",
    "    plt.close('all')\n",
    "    fig = plt.figure(figsize=(16, len(results[os.path.basename(model)])*2))\n",
    "    i = 1\n",
    "\n",
    "    for subject, subject_channels, subject_label, pred_label, ~ in results[os.path.basename(model)]:\n",
    "        original_img = nib.load(subject_channels[0])\n",
    "        original_label_img = nib.load(subject_label)\n",
    "        predicted_label_img = nib.load(pred_label)\n",
    "        \n",
    "        ax = fig.add_subplot(len(results[os.path.basename(model)]), 2, i)\n",
    "        temp = plot_roi(original_label_img, original_img, display_mode='z', cut_coords=4, figure=fig, axes=ax)\n",
    "        ax = fig.add_subplot(len(results[os.path.basename(model)]), 2, i+1)\n",
    "        plot_roi(predicted_label_img, original_img, display_mode='z', cut_coords=temp.cut_coords, figure=fig, axes=ax)\n",
    "        i += 2\n",
    "\n",
    "    plt.subplots_adjust(wspace=0, hspace=0)\n",
    "    plt.savefig(os.path.join(model, 'testSegResults_' + os.path.basename(model) + '.pdf'), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PROCESS TRAINING AND VALIDATION RESULTS**\n",
    "\n",
    "Plot and save training progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in trained_models:\n",
    "    # Plot and save training progress\n",
    "    os.system(\"python ischleseg/deepmedic/plotSaveTrainingProgress.py \" +\n",
    "              os.path.join(model, \"output/logs/trainSession.txt -d -m 20 -s\"))\n",
    "    # Move files to the corresponding model directory\n",
    "    os.system(\"mv trainingProgress.pdf \" + os.path.join(model, 'trainingProgress_' + os.path.basename(model) + '.pdf'))\n",
    "    os.system(\"mv trainingProgress.txt \" + os.path.join(model, 'trainingProgress.txt'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load training metrics and compute mean and variance between models (includes training and validation metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load \"measuredMetricsFromAllExperiments\"\n",
    "# 1st dimension: \"Validation\" (0), \"Training\" (1)\n",
    "# 2nd dimension: ? (0)\n",
    "# 3rd dimension: \"Mean Accuracy\" (0), \"Sensitivity\" (1), \"Specificity\" (2), \"DSC (samples)\" (3), \"DSC (full-segm)\" (4)\n",
    "\n",
    "metrics = {}\n",
    "for model in trained_models:\n",
    "    with open(os.path.join(model, 'trainingProgress.txt'), 'rb') as input:\n",
    "        metrics[os.path.basename(model)] = pickle.load(input)\n",
    "        \n",
    "# Compute mean and variance of all models' variations metrics\n",
    "metrics_mean = {}\n",
    "metrics_var = {}\n",
    "metrics_values = np.array(metrics.values())\n",
    "metrics_names_0 = ['Validation', 'Training']\n",
    "metrics_names_1 = ['Mean Accuracy', 'Sensitivity', 'Specificity', 'DSC (Samples)', 'DSC (full-segm)']\n",
    "\n",
    "for i in range(len(metrics_names_0)):\n",
    "    metrics_mean[metrics_names_0[i]] = {}\n",
    "    metrics_var[metrics_names_0[i]] = {}\n",
    "    for j in range(len(metrics_names_1)):\n",
    "        if i == 1 and j == 4: # Skip DSC_full for training (is never calculated)\n",
    "            metrics_mean[metrics_names_0[i]][metrics_names_1[j]] = np.zeros(35*20)\n",
    "            metrics_var[metrics_names_0[i]][metrics_names_1[j]] = np.zeros(35*20)\n",
    "            continue \n",
    "        metrics_mean[metrics_names_0[i]][metrics_names_1[j]] = np.mean(metrics_values[:,i,0,j])\n",
    "        metrics_var[metrics_names_0[i]][metrics_names_1[j]] = np.var(metrics_values[:,i,0,j])        \n",
    "\n",
    "train_val_metrics = {}\n",
    "train_val_metrics['mean'] = metrics_mean\n",
    "train_val_metrics['var'] = metrics_var\n",
    "# Save final experiment metrics\n",
    "with open(os.path.join(root, model_variant + '_train_val_metrics.txt'), 'wb') as output:\n",
    "    pickle.dump(train_val_metrics, output, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot mean training and validation metrics of all trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all')\n",
    "rows, cols = [2, 5]\n",
    "fig = plt.figure(figsize=(cols*6, rows*4))\n",
    "for i in range(len(metrics_names_0)):\n",
    "    if i == 0: continue # Skip validation data (models in milestones_3 did no validation)\n",
    "    for j in range(len(metrics_names_1)):\n",
    "        ax = fig.add_subplot(rows, cols, i * cols + 1 + j)\n",
    "        plt.plot(np.arange(0, 35, 1/20.0), metrics_mean[metrics_names_0[i]][metrics_names_1[j]], 'r')\n",
    "        plt.xlim(0, 35)\n",
    "        plt.ylim(0, 1.0)\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel(metrics_names_0[i])\n",
    "        plt.title(metrics_names_1[j])\n",
    "\n",
    "# Save mean training and validation metrics of all trained models\n",
    "plt.savefig(os.path.join(root, model_variant + 'meanTrainProgress.pdf'), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################\n",
    "##### POSTPROCESSING FOR TRAINING + VALIDATION SETS MODELS ######\n",
    "#################################################################\n",
    "\n",
    "# load prediction\n",
    "root_data = './data/ISLES2017/training'\n",
    "\n",
    "root = \"./milestones_1\"\n",
    "trained_models = sorted(glob(os.path.join(root, '*DM_V[0-1]_*[0-4]')))\n",
    "results = {}\n",
    "\n",
    "for model in trained_models:\n",
    "    root = os.path.join(model, 'output/predictions/trainSession/predictions')\n",
    "    \n",
    "    segms = sorted(glob(os.path.join(root, '*Segm.nii.gz')))\n",
    "    prob_maps_class0 = sorted(glob(os.path.join(root, '*ProbMapClass0.nii.gz')))\n",
    "    prob_maps_class1 = sorted(glob(os.path.join(root, '*ProbMapClass1.nii.gz')))\n",
    "    results[os.path.basename(model)] = []\n",
    "    \n",
    "    # resize its prediction for final result validation\n",
    "    for i in range(len(segms)):\n",
    "        # Find subject that contains the code in pred.\n",
    "        subject = sorted([y\n",
    "                          for x in os.walk(root_data)\n",
    "                          for y in glob(os.path.join(x[0], '*'))\n",
    "                          if os.path.basename(segms[i]).split('_')[-2].split('.')[-1] in y\n",
    "                         ])[0].split('/')[-2]\n",
    "\n",
    "        subject_channels = sorted([y\n",
    "                                   for x in os.walk(os.path.join(root_data, subject))\n",
    "                                   for y in glob(os.path.join(x[0], '*MR_*.nii'))\n",
    "                                   if '4DPWI' not in y\n",
    "                                  ])\n",
    "        subject_label = sorted([y\n",
    "                                for x in os.walk(os.path.join(root_data, subject))\n",
    "                                for y in glob(os.path.join(x[0], '*OT*.nii'))\n",
    "                               ])\n",
    "\n",
    "        # Load ADC channel as reference\n",
    "        original_img = nib.load(subject_channels[0])\n",
    "\n",
    "        # load predictions\n",
    "        pred = nib.load(segms[i])\n",
    "        pmap_0 = nib.load(prob_maps_class0[i])\n",
    "        pmap_1 = nib.load(prob_maps_class1[i])\n",
    "\n",
    "        # Upsample to original size\n",
    "        pred = resample_img(pred,\n",
    "                            original_img.affine,\n",
    "                            original_img.shape,\n",
    "                            interpolation='nearest')\n",
    "        pmap_0 = resample_img(pmap_0,\n",
    "                              original_img.affine,\n",
    "                              original_img.shape,\n",
    "                              interpolation='continuous')\n",
    "        pmap_1 = resample_img(pmap_1,\n",
    "                              original_img.affine,\n",
    "                              original_img.shape,\n",
    "                              interpolation='continuous')        \n",
    "        \n",
    "        results[os.path.basename(model)].append([subject_channels, subject_label[0], pred, pmap_0, pmap_1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flag for testSession_0 or testSession_1\n",
    "sflag = 0\n",
    "\n",
    "if sflag:\n",
    "    session = 'testSession_1'\n",
    "    root_data = './data/ISLES2017/testing'\n",
    "else:\n",
    "    session = 'testSession_0'\n",
    "    root_data = './data/ISLES2017/training'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################\n",
    "##### POSTPROCESSING FOR TRAINING + TEST SETS MODELS ######\n",
    "###########################################################\n",
    "\n",
    "root = \"./milestones_3\"\n",
    "trained_models = sorted(glob(os.path.join(root, '*DM_V[2-3]')))\n",
    "results = {}\n",
    "\n",
    "for model in trained_models:\n",
    "    root = os.path.join(model, 'output/predictions/' + session + '/predictions')\n",
    "    root_2 = os.path.join(model, 'output/predictions/' + session)\n",
    "    \n",
    "    # load predictions\n",
    "    # resample predictions to original size\n",
    "    # save predictions (as .nii) matching SMIR required format\n",
    "    \n",
    "    preds = sorted(glob(os.path.join(root, '*Segm.nii.gz')))\n",
    "\n",
    "    results[os.path.basename(model)] = []\n",
    "    \n",
    "    # resize its prediction for final result validation\n",
    "    for i in range(len(preds)):\n",
    "        # Find subject that contains the code in pred.\n",
    "        subject = sorted([y\n",
    "                          for x in os.walk(root_data)\n",
    "                          for y in glob(os.path.join(x[0], '*'))\n",
    "                          if os.path.basename(preds[i]).split('_')[-2].split('.')[-1] in y\n",
    "                         ])[0].split('/')[-2]\n",
    "\n",
    "        subject_channels = sorted([y\n",
    "                                   for x in os.walk(os.path.join(root_data, subject))\n",
    "                                   for y in glob(os.path.join(x[0], '*MR_*.nii'))\n",
    "                                   if '4DPWI' not in y\n",
    "                                  ])\n",
    "\n",
    "        # Load ADC channel as reference\n",
    "        original_img = nib.load(subject_channels[0])\n",
    "\n",
    "        # load prediction\n",
    "        pred = nib.load(preds[i])\n",
    "\n",
    "        # Upsample to original size\n",
    "        pred = resample_img(pred,\n",
    "                            original_img.affine,\n",
    "                            original_img.shape,\n",
    "                            interpolation='nearest')\n",
    "        \n",
    "        # Save prediction\n",
    "        nib.save(pred, os.path.join(root_2, os.path.basename(preds[i]).split('_')[-2] + '.nii'))\n",
    "        \n",
    "        results[os.path.basename(model)].append([subject, subject_channels, pred])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
