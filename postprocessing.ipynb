{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Imports\n",
    "import os\n",
    "import shutil\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import scipy.stats as st\n",
    "import nilearn\n",
    "\n",
    "from matplotlib.colors import colorConverter\n",
    "from glob import glob\n",
    "from scipy import ndimage\n",
    "from nilearn.image import resample_to_img, resample_img\n",
    "from nilearn.masking import compute_background_mask, compute_epi_mask\n",
    "from nilearn.plotting import plot_roi, plot_epi, plot_img, plot_anat\n",
    "from scipy.spatial.distance import directed_hausdorff\n",
    "from nipype.algorithms.metrics import Distance\n",
    "from sklearn.metrics import roc_curve, auc, f1_score, precision_recall_curve, average_precision_score\n",
    "from scipy import interp\n",
    "from itertools import chain\n",
    "from scipy.ndimage.morphology import binary_dilation, binary_erosion, binary_closing, binary_opening\n",
    "from skimage.morphology import cube, octahedron, ball"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(original_path, predicted_path, mask_path, th=None):\n",
    "    original_data = nib.load(original_path).get_data()\n",
    "    predicted_data = nib.load(predicted_path).get_data()\n",
    "    mask_data = nib.load(mask_path).get_data() # use mask to limit results to the brain\n",
    "    \n",
    "    # Threshold data if necessary\n",
    "    if th is not None:\n",
    "        predicted_data = (predicted_data > th).astype(int)\n",
    "    \n",
    "    metrics = {}\n",
    "    \n",
    "    # Real positive cases\n",
    "    metrics['P'] = float(np.sum((original_data == 1).astype(int) * mask_data))\n",
    "\n",
    "    # Real negative cases\n",
    "    metrics['N'] = float(np.sum((original_data == 0).astype(int) * mask_data))\n",
    "\n",
    "    # True positive\n",
    "    metrics['TP'] = float(np.sum((((predicted_data == 1).astype(int) +\n",
    "                                   (original_data == 1).astype(int)) * mask_data) == 2))\n",
    "\n",
    "    # True negative\n",
    "    metrics['TN'] = float(np.sum((((predicted_data == 0).astype(int) +\n",
    "                                   (original_data == 0).astype(int)) * mask_data) == 2))\n",
    "\n",
    "    # False positive (all 1's in predicted minus original 1's)\n",
    "    metrics['FP'] = float(np.sum((((predicted_data == 1).astype(int) -\n",
    "                                   (original_data == 1).astype(int)) * mask_data) == 1))\n",
    "\n",
    "    # False negative\n",
    "    metrics['FN'] = float(np.sum((((predicted_data == 1).astype(int) -\n",
    "                                   (original_data == 1).astype(int)) * mask_data) == -1))\n",
    "\n",
    "    # True positive rate (Sensitivity, Recall)\n",
    "    metrics['TPR'] = metrics['TP'] / (metrics['TP'] + metrics['FN'])  \n",
    "\n",
    "    # True negative rate (Specificity)\n",
    "    metrics['TNR'] = metrics['TN'] / (metrics['TN'] + metrics['FP'])\n",
    "\n",
    "    # Positive predictive value (Precision)\n",
    "    metrics['PPV'] = metrics['TP'] / (metrics['TP'] + metrics['FP'])\n",
    "\n",
    "    # Negative predictive value\n",
    "    metrics['NPV'] = metrics['TN'] / (metrics['TN'] + metrics['FN'])\n",
    "\n",
    "    # False negative rate (Miss rate)\n",
    "    metrics['FNR'] = 1 -  metrics['TPR']\n",
    "\n",
    "    # False positive rate (Fall-out)\n",
    "    metrics['FPR'] = 1 - metrics['TNR']\n",
    "\n",
    "    # False discovery rate\n",
    "    metrics['FDR'] = 1 - metrics['PPV']\n",
    "\n",
    "    # False omission rate\n",
    "    metrics['FOR'] = 1 - metrics['NPV']\n",
    "\n",
    "    # Accuracy\n",
    "    metrics['ACC'] = (metrics['TP'] + metrics['TN']) / \\\n",
    "                                (metrics['TP'] + \n",
    "                                 metrics['TN'] + \n",
    "                                 metrics['FP'] + \n",
    "                                 metrics['FN'])\n",
    "\n",
    "    # F1 Score (also known as DSC, Sørensen–Dice coefficient, ...)\n",
    "    #metrics['F1S'] = 2 * (metrics['PPV'] * metrics['TPR']) / \\\n",
    "    #                                (metrics['PPV'] + metrics['TPR'])\n",
    "    metrics['F1S'] = (2*metrics['TP']) / (2*metrics['TP'] + metrics['FP'] + metrics['FN'])\n",
    "\n",
    "    # Matthews correlation coefficient\n",
    "    # The MCC can be more appropriate when negatives actually mean something,\n",
    "    # and can be more useful in other ways.\n",
    "    metrics['MCC'] = ((metrics['TP'] * metrics['TN']) - (metrics['FP'] * metrics['FN'])) / \\\n",
    "                        np.sqrt(\n",
    "                            (metrics['TP'] + metrics['FP']) *\n",
    "                            (metrics['TP'] + metrics['FN']) *\n",
    "                            (metrics['TN'] + metrics['FP']) *\n",
    "                            (metrics['TN'] + metrics['FN']))\n",
    "\n",
    "    # Compute Hausdorff distance\n",
    "    D = Distance()\n",
    "    if th is not None:\n",
    "        try:\n",
    "            metrics['HD'] = D._eucl_max(nib.load(original_path),\n",
    "                                        nib.Nifti1Image(predicted_data, nib.load(predicted_path).affine))\n",
    "        except:\n",
    "            metrics['HD'] = float('nan')\n",
    "    else:\n",
    "        metrics['HD'] = D._eucl_max(nib.load(original_path), nib.load(predicted_path))\n",
    "\n",
    "    # Compute Jaccard index\n",
    "    metrics['JI'] = metrics['TP'] / (metrics['FN'] + metrics['FP'] + metrics['TP'])\n",
    "\n",
    "    # Informedness or Bookmaker informedness\n",
    "    metrics['BM'] = metrics['TPR'] + metrics['TNR'] - 1\n",
    "\n",
    "    #Markedness\n",
    "    metrics['MK'] = metrics['PPV'] + metrics['NPV'] - 1\n",
    "\n",
    "    \n",
    "    return(metrics)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set working directory\n",
    "os.chdir('/home/uziel/DISS')\n",
    "# Set root of models to be post-processed\n",
    "root = \"./milestones_6\"\n",
    "model_variant = 'DM_V0_R_[0-4]' # choose model variant. Eg. \"DM_V0_[0-4]\".\n",
    "tmp = model_variant.split('_')\n",
    "if len(tmp) == 3:\n",
    "    model_name = tmp[1]\n",
    "elif len(tmp) == 4:\n",
    "    model_name = tmp[1] + '_' + tmp[2]\n",
    "else:\n",
    "    model_name = tmp[1] + '_' + tmp[2] + '_' + tmp[3]\n",
    "    \n",
    "# Load all trained models (k-folds) of model_variant\n",
    "trained_models = sorted(glob(os.path.join(root, model_variant)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**POSTPROCESSING** for test cases\n",
    "\n",
    "Upsample predicted labels and compute test metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################\n",
    "##### POSTPROCESSING FOR K-FOLD CROSS-VALIDATION MODELS (0) ######\n",
    "##################################################################\n",
    "\n",
    "root_data = './data/ISLES2017/training'\n",
    "root_data_processed = './data_processed/ISLES2017/training'\n",
    "test_results = {}\n",
    "\n",
    "for model in trained_models:\n",
    "    root_1 = os.path.join(model, 'output/predictions/testSession/predictions')\n",
    "    root_2 = os.path.dirname(root_1)\n",
    "    \n",
    "    # Load label predictions\n",
    "    preds = sorted(glob(os.path.join(root_1, '*Segm.nii.gz')))\n",
    "    # Load probability maps of background\n",
    "    pmap_0 = sorted(glob(os.path.join(root_1, '*ProbMapClass0.nii.gz')))\n",
    "    # Load probability maps of foreground\n",
    "    pmap_1 = sorted(glob(os.path.join(root_1, '*ProbMapClass1.nii.gz')))\n",
    "    \n",
    "    test_results[os.path.basename(model)] = []\n",
    "    \n",
    "    # resize its prediction for final result validation\n",
    "    for i in range(len(preds)):\n",
    "        # Find subject that contains the code in pred.\n",
    "        subject = sorted([y\n",
    "                          for x in os.walk(root_data)\n",
    "                          for y in glob(os.path.join(x[0], '*'))\n",
    "                          if os.path.basename(preds[i]).split('_')[-2].split('.')[-1] in y\n",
    "                         ])[0].split('/')[-2]\n",
    "\n",
    "        subject_channels = sorted([y\n",
    "                                   for x in os.walk(os.path.join(root_data, subject))\n",
    "                                   for y in glob(os.path.join(x[0], '*MR_*.nii'))\n",
    "                                   if '4DPWI' not in y\n",
    "                                  ])\n",
    "        \n",
    "        subject_label = sorted([y\n",
    "                                for x in os.walk(os.path.join(root_data, subject))\n",
    "                                for y in glob(os.path.join(x[0], '*OT*.nii'))\n",
    "                               ])[0]\n",
    "\n",
    "        subject_processed = sorted([y\n",
    "                                    for x in os.walk(root_data_processed)\n",
    "                                    for y in glob(os.path.join(x[0], '*'))\n",
    "                                    if os.path.basename(preds[i]).split('_')[-2].split('.')[-1] in y\n",
    "                                   ])[0].split('/')[-2]\n",
    "        \n",
    "        subject_mask = sorted([y\n",
    "                               for x in os.walk(os.path.join(root_data_processed, subject_processed))\n",
    "                               for y in glob(os.path.join(x[0], '*mask*'))\n",
    "                              ])[0]\n",
    "        \n",
    "        # Load ADC channel as reference\n",
    "        original_img = nib.load(subject_channels[0])\n",
    "\n",
    "        # Load predictions and prob maps\n",
    "        pred_img = nib.load(preds[i])\n",
    "        pmap_0_img = nib.load(pmap_0[i])\n",
    "        pmap_1_img = nib.load(pmap_1[i])\n",
    "        \n",
    "        # Upsample to original size\n",
    "        pred_img = resample_img(pred_img,\n",
    "                                original_img.affine,\n",
    "                                original_img.shape,\n",
    "                                interpolation='nearest')\n",
    "        \n",
    "        pmap_0_img = resample_img(pmap_0_img,\n",
    "                                  original_img.affine,\n",
    "                                  original_img.shape,\n",
    "                                  interpolation='continuous')\n",
    "        \n",
    "        pmap_1_img = resample_img(pmap_1_img,\n",
    "                                  original_img.affine,\n",
    "                                  original_img.shape,\n",
    "                                  interpolation='continuous')\n",
    "        \n",
    "        # Load subject mask\n",
    "        mask_img = nib.load(subject_mask)\n",
    "        \n",
    "        # Upsample to original size\n",
    "        mask_img = resample_img(mask_img,\n",
    "                                original_img.affine,\n",
    "                                original_img.shape,\n",
    "                                interpolation='nearest')\n",
    "        \n",
    "        # Save prediction\n",
    "        pred_path = os.path.join(root_2, \"_\".join(os.path.basename(preds[i]).split('_')[:-1]) + '.pred.nii')\n",
    "        pmap_0_path = os.path.join(root_2, \"_\".join(os.path.basename(pmap_0[i]).split('_')[:-1]) + '.pmap_0.nii')\n",
    "        pmap_1_path = os.path.join(root_2, \"_\".join(os.path.basename(pmap_1[i]).split('_')[:-1]) + '.pmap_1.nii')\n",
    "        mask_path = os.path.join(root_2, \"_\".join(os.path.basename(pmap_1[i]).split('_')[:-1]) + '.mask.nii')\n",
    "        \n",
    "        nib.save(pred_img, pred_path)\n",
    "        nib.save(pmap_0_img, pmap_0_path)\n",
    "        nib.save(pmap_1_img, pmap_1_path)\n",
    "        nib.save(mask_img, mask_path)\n",
    "        \n",
    "        # Compute metrics between original and predicted label\n",
    "        metrics = get_metrics(subject_label, pred_path, mask_path)\n",
    "        \n",
    "        test_results[os.path.basename(model)].append([subject,\n",
    "                                                      subject_channels,\n",
    "                                                      subject_label,\n",
    "                                                      pred_path,\n",
    "                                                      pmap_0_path,\n",
    "                                                      pmap_1_path,\n",
    "                                                      mask_path,\n",
    "                                                      metrics])\n",
    "        \n",
    "    # Save model results\n",
    "    with open(os.path.join(model, 'test_results.pkl'), 'wb') as output:\n",
    "        pickle.dump(test_results[os.path.basename(model)], output, pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "    # Compute mean and std of model subjects predictions' metrics\n",
    "    metrics = np.array(test_results[os.path.basename(model)])[:,7]\n",
    "    test_metrics = {}\n",
    "    test_metrics['mean'] = {k : np.mean([t[k] for t in metrics]) for k in metrics[0]}\n",
    "    test_metrics['std'] = {k : np.std([t[k] for t in metrics]) for k in metrics[0]}\n",
    "    \n",
    "    # Save each model's metrics\n",
    "    with open(os.path.join(model, 'test_metrics.pkl'), 'wb') as output:\n",
    "        pickle.dump(test_metrics, output, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# Save all models' results\n",
    "with open(os.path.join(root, model_name + '_test_results.pkl'), 'wb') as output:\n",
    "    pickle.dump(test_results, output, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load each model's metrics, compute mean and std. This is the final result of an experiment, and determines its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = []\n",
    "for model in trained_models:\n",
    "    with open(os.path.join(model, 'test_metrics.pkl'), 'rb') as input:\n",
    "        metrics.append(pickle.load(input))\n",
    "\n",
    "metrics = np.array(metrics)\n",
    "test_metrics['mean'] = {k : np.mean([t['mean'][k] for t in metrics]) for k in metrics[0]['mean']}\n",
    "test_metrics['std'] = {k : np.std([t['std'][k] for t in metrics]) for k in metrics[0]['std']}\n",
    "\n",
    "# Save final experiment metrics\n",
    "with open(os.path.join(root, model_name + '_test_metrics.pkl'), 'wb') as output:\n",
    "    pickle.dump(test_metrics, output, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot original and predicted labels for test cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot original label and predicted label on top of original image\n",
    "for model in trained_models:\n",
    "    \n",
    "    plt.close('all')\n",
    "    fig = plt.figure(figsize=(15, int(len(test_results[os.path.basename(model)])*2.5)))\n",
    "    i = 1\n",
    "\n",
    "    for subject, subject_channels, subject_label, pred_label, _, _, _, metrics in test_results[os.path.basename(model)]:\n",
    "        original_img = nib.load(subject_channels[0])\n",
    "        original_label_img = nib.load(subject_label)\n",
    "        predicted_label_img = nib.load(pred_label)\n",
    "        \n",
    "        ax = fig.add_subplot(len(test_results[os.path.basename(model)]), 2, i)\n",
    "        ax.set_title('Ground truth for subject: ' + subject,\n",
    "                    color='w')\n",
    "        temp = plot_roi(original_label_img, original_img, display_mode='z', cut_coords=4, figure=fig, axes=ax)\n",
    "        ax = fig.add_subplot(len(test_results[os.path.basename(model)]), 2, i+1)\n",
    "        ax.set_title('Prediction. DICE: %.2f, HD: %.2f, JI: %.2f, SEN: %.2f, SPE: %.2f.'\n",
    "                             % (metrics['F1S'], metrics['HD'], metrics['JI'], metrics['TPR'], metrics['TNR']),\n",
    "                    color='w')\n",
    "        plot_roi(predicted_label_img, original_img, display_mode='z', cut_coords=temp.cut_coords, figure=fig, axes=ax)\n",
    "        i += 2\n",
    "\n",
    "    fig.patch.set_facecolor('xkcd:black')\n",
    "    fig.savefig(os.path.join(model, 'testSegResults_' + os.path.basename(model) + '.pdf'),\n",
    "                bbox_inches='tight',\n",
    "                facecolor=fig.get_facecolor())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ROC CURVE** for test cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mean_fpr = {}\n",
    "model_mean_tpr = {}\n",
    "model_mean_auc = {}\n",
    "for model in trained_models:\n",
    "    original_data = []\n",
    "    predicted_data = []\n",
    "    for _, _, subject_label, _, _, pmap_1_path, _, _ in test_results[os.path.basename(model)]:\n",
    "        original_data.append(nib.load(subject_label).get_data().ravel())\n",
    "        predicted_data.append(nib.load(pmap_1_path).get_data().ravel())\n",
    "\n",
    "    # Join all subjects to perform micro-average\n",
    "    y_true = list(chain.from_iterable(original_data))\n",
    "    y_pred = list(chain.from_iterable(predicted_data))\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_pred)\n",
    "    \n",
    "    model_mean_fpr[os.path.basename(model)] = fpr\n",
    "    model_mean_tpr[os.path.basename(model)] = tpr\n",
    "    model_mean_auc[os.path.basename(model)] = auc(fpr, tpr)\n",
    "    \n",
    "plt.figure()\n",
    "lw = 2\n",
    "tprs = []\n",
    "aucs = []\n",
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "for model in trained_models:\n",
    "    tprs.append(interp(mean_fpr, model_mean_fpr[os.path.basename(model)], model_mean_tpr[os.path.basename(model)]))\n",
    "    aucs.append(model_mean_auc[os.path.basename(model)])\n",
    "    plt.plot(model_mean_fpr[os.path.basename(model)], model_mean_tpr[os.path.basename(model)], lw=1, alpha=0.3,\n",
    "             label = 'Fold {0} (AUC = {1:0.2f})'\n",
    "             ''.format(os.path.basename(model).split('_')[-1], model_mean_auc[os.path.basename(model)]))\n",
    "\n",
    "\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',\n",
    "         label='Luck', alpha=.8)\n",
    "plt.plot([0, 1], [1, 0], 'k:', lw=lw, label = 'EER')\n",
    "\n",
    "mean_tpr = np.mean(tprs, axis=0)\n",
    "mean_tpr[-1] = 1.0\n",
    "mean_auc = auc(mean_fpr, mean_tpr)\n",
    "std_auc = np.std(aucs)\n",
    "plt.plot(mean_fpr, mean_tpr, color='b',\n",
    "         label=r'Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc, std_auc),\n",
    "         lw=2, alpha=.8)\n",
    "\n",
    "std_tpr = np.std(tprs, axis=0)\n",
    "tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
    "tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
    "plt.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2,\n",
    "                 label=r'$\\pm$ 1 std. dev.')\n",
    "\n",
    "plt.xlim([-0.05, 1.05])\n",
    "plt.ylim([-0.05, 1.05])\n",
    "\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic curve (' + model_name + ')')\n",
    "plt.legend(loc='lower right')\n",
    "plt.savefig(os.path.join(root, model_name + '_test_roc.pdf'), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PRECISION-RECALL CURVE** for test cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mean_precision = {}\n",
    "model_mean_recall = {}\n",
    "model_mean_ap = {}\n",
    "for model in trained_models:\n",
    "    original_data = []\n",
    "    predicted_data = []\n",
    "    for _, _, subject_label, _, _, pmap_1_path, _, _ in test_results[os.path.basename(model)]:\n",
    "        original_data.append(nib.load(subject_label).get_data().ravel())\n",
    "        predicted_data.append(nib.load(pmap_1_path).get_data().ravel())\n",
    "\n",
    "    # Join all subjects to perform micro-average\n",
    "    y_true = list(chain.from_iterable(original_data))\n",
    "    y_pred = list(chain.from_iterable(predicted_data))\n",
    "    precision, recall, _ = precision_recall_curve(y_true, y_pred)\n",
    "    average_precision = average_precision_score(y_true, y_pred)\n",
    "    \n",
    "    model_mean_precision[os.path.basename(model)] = precision\n",
    "    model_mean_recall[os.path.basename(model)] = recall\n",
    "    model_mean_ap[os.path.basename(model)] = average_precision\n",
    "\n",
    "plt.figure(figsize=(7, 8))\n",
    "\n",
    "f_scores = np.linspace(0.2, 0.8, num=4)\n",
    "lines = []\n",
    "labels = []\n",
    "for f_score in f_scores:\n",
    "    x = np.linspace(0.01, 1)\n",
    "    y = f_score * x / (2 * x - f_score)\n",
    "    l, = plt.plot(x[y >= 0], y[y >= 0], color='gray', alpha=0.2)\n",
    "    plt.annotate('F1S={0:0.1f}'.format(f_score), xy=(0.85, y[45] + 0.02))\n",
    "    \n",
    "lines.append(l)\n",
    "labels.append('iso-F1S curves')\n",
    "lw = 2\n",
    "for model in trained_models:\n",
    "    l, = plt.plot(model_mean_recall[os.path.basename(model)],\n",
    "                  model_mean_precision[os.path.basename(model)],\n",
    "                  lw=lw)\n",
    "    lines.append(l)\n",
    "    labels.append('Fold {0} (AP = {1:0.2f})'\n",
    "                  ''.format(os.path.basename(model).split('_')[-1],\n",
    "                            model_mean_ap[os.path.basename(model)]))\n",
    "\n",
    "fig = plt.gcf()\n",
    "fig.subplots_adjust(bottom=0.25)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.xlabel('True Positive Rate (Recall)')\n",
    "plt.ylabel('Positive Predictive Value (Precision)')\n",
    "plt.title('Precision-Recall curve (' + model_name + ')')\n",
    "plt.legend(lines, labels, loc='lower left')\n",
    "plt.savefig(os.path.join(root, model_name + '_test_pr.pdf'), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**POSTPROCESSING** for validation cases\n",
    "\n",
    "Upsample predicted labels and compute validation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################\n",
    "##### POSTPROCESSING FOR K-FOLD CROSS-VALIDATION MODELS (0) ######\n",
    "##################################################################\n",
    "\n",
    "root_data = './data/ISLES2017/training'\n",
    "root_data_processed = './data_processed/ISLES2017/training'\n",
    "val_results = {}\n",
    "\n",
    "for model in trained_models:\n",
    "    root_1 = os.path.join(model, 'output/predictions/valSession/predictions')\n",
    "    root_2 = os.path.dirname(root_1)\n",
    "    \n",
    "    # Load label predictions\n",
    "    preds = sorted(glob(os.path.join(root_1, '*Segm.nii.gz')))\n",
    "    # Load probability maps of background\n",
    "    pmap_0 = sorted(glob(os.path.join(root_1, '*ProbMapClass0.nii.gz')))\n",
    "    # Load probability maps of foreground\n",
    "    pmap_1 = sorted(glob(os.path.join(root_1, '*ProbMapClass1.nii.gz')))\n",
    "    \n",
    "    val_results[os.path.basename(model)] = []\n",
    "    \n",
    "    # resize its prediction for final result validation\n",
    "    for i in range(len(preds)):\n",
    "        # Find subject that contains the code in pred.\n",
    "        subject = sorted([y\n",
    "                          for x in os.walk(root_data)\n",
    "                          for y in glob(os.path.join(x[0], '*'))\n",
    "                          if os.path.basename(preds[i]).split('_')[-2].split('.')[-1] in y\n",
    "                         ])[0].split('/')[-2]\n",
    "\n",
    "        subject_channels = sorted([y\n",
    "                                   for x in os.walk(os.path.join(root_data, subject))\n",
    "                                   for y in glob(os.path.join(x[0], '*MR_*.nii'))\n",
    "                                   if '4DPWI' not in y\n",
    "                                  ])\n",
    "        \n",
    "        subject_label = sorted([y\n",
    "                                for x in os.walk(os.path.join(root_data, subject))\n",
    "                                for y in glob(os.path.join(x[0], '*OT*.nii'))\n",
    "                               ])[0]\n",
    "\n",
    "        subject_processed = sorted([y\n",
    "                                    for x in os.walk(root_data_processed)\n",
    "                                    for y in glob(os.path.join(x[0], '*'))\n",
    "                                    if os.path.basename(preds[i]).split('_')[-2].split('.')[-1] in y\n",
    "                                   ])[0].split('/')[-2]\n",
    "        \n",
    "        subject_mask = sorted([y\n",
    "                               for x in os.walk(os.path.join(root_data_processed, subject_processed))\n",
    "                               for y in glob(os.path.join(x[0], '*mask*'))\n",
    "                              ])[0]\n",
    "        \n",
    "        # Load ADC channel as reference\n",
    "        original_img = nib.load(subject_channels[0])\n",
    "\n",
    "        # Load predictions and prob maps\n",
    "        pred_img = nib.load(preds[i])\n",
    "        pmap_0_img = nib.load(pmap_0[i])\n",
    "        pmap_1_img = nib.load(pmap_1[i])\n",
    "        \n",
    "        # Upsample to original size\n",
    "        pred_img = resample_img(pred_img,\n",
    "                                original_img.affine,\n",
    "                                original_img.shape,\n",
    "                                interpolation='nearest')\n",
    "        \n",
    "        pmap_0_img = resample_img(pmap_0_img,\n",
    "                                  original_img.affine,\n",
    "                                  original_img.shape,\n",
    "                                  interpolation='continuous')\n",
    "        \n",
    "        pmap_1_img = resample_img(pmap_1_img,\n",
    "                                  original_img.affine,\n",
    "                                  original_img.shape,\n",
    "                                  interpolation='continuous')\n",
    "        \n",
    "        # Load subject mask\n",
    "        mask_img = nib.load(subject_mask)\n",
    "        \n",
    "        # Upsample to original size\n",
    "        mask_img = resample_img(mask_img,\n",
    "                                original_img.affine,\n",
    "                                original_img.shape,\n",
    "                                interpolation='nearest')\n",
    "        \n",
    "        # Save prediction\n",
    "        pred_path = os.path.join(root_2, \"_\".join(os.path.basename(preds[i]).split('_')[:-1]) + '.pred.nii')\n",
    "        pmap_0_path = os.path.join(root_2, \"_\".join(os.path.basename(pmap_0[i]).split('_')[:-1]) + '.pmap_0.nii')\n",
    "        pmap_1_path = os.path.join(root_2, \"_\".join(os.path.basename(pmap_1[i]).split('_')[:-1]) + '.pmap_1.nii')\n",
    "        mask_path = os.path.join(root_2, \"_\".join(os.path.basename(pmap_1[i]).split('_')[:-1]) + '.mask.nii')\n",
    "        \n",
    "        nib.save(pred_img, pred_path)\n",
    "        nib.save(pmap_0_img, pmap_0_path)\n",
    "        nib.save(pmap_1_img, pmap_1_path)\n",
    "        nib.save(mask_img, mask_path)\n",
    "        \n",
    "        # Compute metrics between original and predicted label\n",
    "        metrics = get_metrics(subject_label, pred_path, mask_path)\n",
    "        \n",
    "        val_results[os.path.basename(model)].append([subject,\n",
    "                                                     subject_channels,\n",
    "                                                     subject_label,\n",
    "                                                     pred_path,\n",
    "                                                     pmap_0_path,\n",
    "                                                     pmap_1_path,\n",
    "                                                     mask_path,\n",
    "                                                     metrics])\n",
    "        \n",
    "    # Save model results\n",
    "    with open(os.path.join(model, 'val_results.pkl'), 'wb') as output:\n",
    "        pickle.dump(val_results[os.path.basename(model)], output, pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "    # Compute mean and std of model subjects predictions' metrics\n",
    "    metrics = np.array(val_results[os.path.basename(model)])[:,7]\n",
    "    val_metrics = {}\n",
    "    val_metrics['mean'] = {k : np.mean([t[k] for t in metrics]) for k in metrics[0]}\n",
    "    val_metrics['std'] = {k : np.std([t[k] for t in metrics]) for k in metrics[0]}\n",
    "    \n",
    "    # Save each model's metrics\n",
    "    with open(os.path.join(model, 'val_metrics.pkl'), 'wb') as output:\n",
    "        pickle.dump(val_metrics, output, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# Save all models' results\n",
    "with open(os.path.join(root, model_name + '_val_results.pkl'), 'wb') as output:\n",
    "    pickle.dump(val_results, output, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load each model's metrics, compute mean and std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = []\n",
    "for model in trained_models:\n",
    "    with open(os.path.join(model, 'val_metrics.pkl'), 'rb') as input:\n",
    "        metrics.append(pickle.load(input))\n",
    "\n",
    "metrics = np.array(metrics)\n",
    "val_metrics['mean'] = {k : np.mean([t['mean'][k] for t in metrics]) for k in metrics[0]['mean']}\n",
    "val_metrics['std'] = {k : np.std([t['std'][k] for t in metrics]) for k in metrics[0]['std']}\n",
    "\n",
    "# Save final experiment metrics\n",
    "with open(os.path.join(root, model_name + '_val_metrics.pkl'), 'wb') as output:\n",
    "    pickle.dump(val_metrics, output, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot original and predicted labels for validation cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot original label and predicted label on top of original image\n",
    "for model in trained_models:\n",
    "    \n",
    "    plt.close('all')\n",
    "    fig = plt.figure(figsize=(16, len(val_results[os.path.basename(model)])*2))\n",
    "    i = 1\n",
    "\n",
    "    for subject, subject_channels, subject_label, pred_label, _, _, _, metrics in val_results[os.path.basename(model)]:\n",
    "        original_img = nib.load(subject_channels[0])\n",
    "        original_label_img = nib.load(subject_label)\n",
    "        predicted_label_img = nib.load(pred_label)\n",
    "        \n",
    "        ax = fig.add_subplot(len(val_results[os.path.basename(model)]), 2, i)\n",
    "        ax.set_title('Ground truth for subject: ' + subject,\n",
    "                    color='w')\n",
    "        temp = plot_roi(original_label_img, original_img, display_mode='z', cut_coords=4, figure=fig, axes=ax)\n",
    "        ax = fig.add_subplot(len(val_results[os.path.basename(model)]), 2, i+1)\n",
    "        ax.set_title('Prediction. DICE: %.2f, HD: %.2f, JI: %.2f, SEN: %.2f, SPE: %.2f.'\n",
    "                             % (metrics['F1S'], metrics['HD'], metrics['JI'], metrics['TPR'], metrics['TNR']),\n",
    "                    color='w')\n",
    "        plot_roi(predicted_label_img, original_img, display_mode='z', cut_coords=temp.cut_coords, figure=fig, axes=ax)\n",
    "        i += 2\n",
    "\n",
    "    fig.patch.set_facecolor('xkcd:black')\n",
    "    fig.savefig(os.path.join(model, 'valSegResults_' + os.path.basename(model) + '.pdf'),\n",
    "                bbox_inches='tight',\n",
    "                facecolor=fig.get_facecolor())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ROC CURVE** for validation cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mean_fpr = {}\n",
    "model_mean_tpr = {}\n",
    "model_mean_auc = {}\n",
    "for model in trained_models:\n",
    "    original_data = []\n",
    "    predicted_data = []\n",
    "    for _, _, subject_label, _, _, pmap_1_path, _, _ in val_results[os.path.basename(model)]:\n",
    "        original_data.append(nib.load(subject_label).get_data().ravel())\n",
    "        predicted_data.append(nib.load(pmap_1_path).get_data().ravel())\n",
    "\n",
    "    # Join all subjects to perform micro-average\n",
    "    y_true = list(chain.from_iterable(original_data))\n",
    "    y_pred = list(chain.from_iterable(predicted_data))\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_pred)\n",
    "    \n",
    "    model_mean_fpr[os.path.basename(model)] = fpr\n",
    "    model_mean_tpr[os.path.basename(model)] = tpr\n",
    "    model_mean_auc[os.path.basename(model)] = auc(fpr, tpr)\n",
    "    \n",
    "plt.figure()\n",
    "lw = 2\n",
    "tprs = []\n",
    "aucs = []\n",
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "for model in trained_models:\n",
    "    tprs.append(interp(mean_fpr, model_mean_fpr[os.path.basename(model)], model_mean_tpr[os.path.basename(model)]))\n",
    "    aucs.append(model_mean_auc[os.path.basename(model)])\n",
    "    plt.plot(model_mean_fpr[os.path.basename(model)], model_mean_tpr[os.path.basename(model)], lw=1, alpha=0.3,\n",
    "             label = 'Fold {0} (AUC = {1:0.2f})'\n",
    "             ''.format(os.path.basename(model).split('_')[-1], model_mean_auc[os.path.basename(model)]))\n",
    "\n",
    "\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',\n",
    "         label='Luck', alpha=.8)\n",
    "plt.plot([0, 1], [1, 0], 'k:', lw=lw, label = 'EER')\n",
    "\n",
    "mean_tpr = np.mean(tprs, axis=0)\n",
    "mean_tpr[-1] = 1.0\n",
    "mean_auc = auc(mean_fpr, mean_tpr)\n",
    "std_auc = np.std(aucs)\n",
    "plt.plot(mean_fpr, mean_tpr, color='b',\n",
    "         label=r'Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc, std_auc),\n",
    "         lw=2, alpha=.8)\n",
    "\n",
    "std_tpr = np.std(tprs, axis=0)\n",
    "tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
    "tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
    "plt.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2,\n",
    "                 label=r'$\\pm$ 1 std. dev.')\n",
    "\n",
    "plt.xlim([-0.05, 1.05])\n",
    "plt.ylim([-0.05, 1.05])\n",
    "\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic curve (' + model_name + ')')\n",
    "plt.legend(loc='lower right')\n",
    "plt.savefig(os.path.join(root, model_name + '_val_roc.pdf'), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PRECISION-RECALL CURVE** for validation cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mean_precision = {}\n",
    "model_mean_recall = {}\n",
    "model_mean_ap = {}\n",
    "for model in trained_models:\n",
    "    original_data = []\n",
    "    predicted_data = []\n",
    "    for _, _, subject_label, _, _, pmap_1_path, _, _ in val_results[os.path.basename(model)]:\n",
    "        original_data.append(nib.load(subject_label).get_data().ravel())\n",
    "        predicted_data.append(nib.load(pmap_1_path).get_data().ravel())\n",
    "\n",
    "    # Join all subjects to perform micro-average\n",
    "    y_true = list(chain.from_iterable(original_data))\n",
    "    y_pred = list(chain.from_iterable(predicted_data))\n",
    "    precision, recall, _ = precision_recall_curve(y_true, y_pred)\n",
    "    average_precision = average_precision_score(y_true, y_pred)\n",
    "    \n",
    "    model_mean_precision[os.path.basename(model)] = precision\n",
    "    model_mean_recall[os.path.basename(model)] = recall\n",
    "    model_mean_ap[os.path.basename(model)] = average_precision\n",
    "\n",
    "plt.figure(figsize=(7, 8))\n",
    "\n",
    "f_scores = np.linspace(0.2, 0.8, num=4)\n",
    "lines = []\n",
    "labels = []\n",
    "for f_score in f_scores:\n",
    "    x = np.linspace(0.01, 1)\n",
    "    y = f_score * x / (2 * x - f_score)\n",
    "    l, = plt.plot(x[y >= 0], y[y >= 0], color='gray', alpha=0.2)\n",
    "    plt.annotate('F1S={0:0.1f}'.format(f_score), xy=(0.85, y[45] + 0.02))\n",
    "    \n",
    "lines.append(l)\n",
    "labels.append('iso-F1S curves')\n",
    "lw = 2\n",
    "for model in trained_models:\n",
    "    l, = plt.plot(model_mean_recall[os.path.basename(model)],\n",
    "                  model_mean_precision[os.path.basename(model)],\n",
    "                  lw=lw)\n",
    "    lines.append(l)\n",
    "    labels.append('Fold {0} (AP = {1:0.2f})'\n",
    "                  ''.format(os.path.basename(model).split('_')[-1],\n",
    "                            model_mean_ap[os.path.basename(model)]))\n",
    "\n",
    "fig = plt.gcf()\n",
    "fig.subplots_adjust(bottom=0.25)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.xlabel('True Positive Rate (Recall)')\n",
    "plt.ylabel('Positive Predictive Value (Precision)')\n",
    "plt.title('Precision-Recall curve (' + model_name + ')')\n",
    "plt.legend(lines, labels, loc='lower left')\n",
    "plt.savefig(os.path.join(root, model_name + '_val_pr.pdf'), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**THRESHOLD TUNING V0**\n",
    "\n",
    "Use precision-recall curve on validation cases to maximize F1S on test cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_test_metrics_tht_v0 = {}\n",
    "for model in trained_models:\n",
    "    # Compute best average threshold for validation cases\n",
    "    original_data = []\n",
    "    predicted_data = []\n",
    "    for _, _, subject_label, _, _, pmap_1_path, mask_path, _ in val_results[os.path.basename(model)]:\n",
    "        original_data.append(nib.load(subject_label).get_data().ravel())\n",
    "        predicted_data.append(nib.load(pmap_1_path).get_data().ravel())\n",
    "\n",
    "    # Join all subjects to perform micro-average\n",
    "    y_true = list(chain.from_iterable(original_data))\n",
    "    y_pred = list(chain.from_iterable(predicted_data))\n",
    "    precision, recall, thresholds = precision_recall_curve(y_true, y_pred)\n",
    "    # Optimal threshold is where precision * recall is maximum\n",
    "    tmp = precision * recall        \n",
    "    # The closest point is the furthest from bottom-left corner\n",
    "    idx = np.argwhere(tmp == np.max(tmp))\n",
    "    th_op = thresholds[idx]\n",
    "    \n",
    "    all_test_metrics_tht_v0[os.path.basename(model)] = []\n",
    "    # Recompute test metrics with optimal threshold from validation cases\n",
    "    for _, _, subject_label, _, _, pmap_1_path, mask_path, _ in test_results[os.path.basename(model)]:\n",
    "        original_data = nib.load(subject_label).get_data()\n",
    "        predicted_data = nib.load(pmap_1_path).get_data()\n",
    "        \n",
    "        # Compute new metrics after new threshold\n",
    "        metrics = get_metrics(subject_label, pmap_1_path, mask_path, th_op)\n",
    "        metrics['TH_OP'] = th_op\n",
    "        all_test_metrics_tht_v0[os.path.basename(model)].append(metrics)\n",
    "        \n",
    "    metrics = np.array(all_test_metrics_tht_v0[os.path.basename(model)])\n",
    "    test_metrics_tht_v0 = {}\n",
    "    test_metrics_tht_v0['mean'] = {k : np.nanmean([t[k] for t in metrics]) for k in metrics[0]}\n",
    "    test_metrics_tht_v0['std'] = {k : np.nanstd([t[k] for t in metrics]) for k in metrics[0]}\n",
    "\n",
    "    with open(os.path.join(model, 'test_metrics_tht_v0.pkl'), 'wb') as output:\n",
    "        pickle.dump(test_metrics_tht_v0, output, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# Save all metrics after threshold tuning v0 for future reference\n",
    "with open(os.path.join(root, model_name +  '_test_results_tht_v0.pkl'), 'wb') as output:\n",
    "    pickle.dump(all_test_metrics_tht_v0, output, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load each model's metrics after **threshold tuning v0**, compute mean and std."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = []\n",
    "for model in trained_models:\n",
    "    with open(os.path.join(model, 'test_metrics_tht_v0.pkl'), 'rb') as input:\n",
    "        metrics.append(pickle.load(input))\n",
    "\n",
    "metrics = np.array(metrics)\n",
    "test_metrics_tht_v0['mean'] = {k : np.nanmean([t['mean'][k] for t in metrics]) for k in metrics[0]['mean']}\n",
    "test_metrics_tht_v0['std'] = {k : np.nanstd([t['std'][k] for t in metrics]) for k in metrics[0]['std']}\n",
    "\n",
    "# Save final experiment metrics after tht_v0\n",
    "with open(os.path.join(root, model_name + '_test_metrics_tht_v0.pkl'), 'wb') as output:\n",
    "    pickle.dump(test_metrics_tht_v0, output, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**THRESHOLD TUNING V1**\n",
    "\n",
    "Use ROC curve on validation cases to maximize BM on test cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_test_metrics_tht_v1 = {}\n",
    "for model in trained_models:\n",
    "    # Compute best average threshold for validation cases\n",
    "    original_data = []\n",
    "    predicted_data = []\n",
    "    for _, _, subject_label, _, _, pmap_1_path, mask_path, _ in val_results[os.path.basename(model)]:\n",
    "        original_data.append(nib.load(subject_label).get_data().ravel())\n",
    "        predicted_data.append(nib.load(pmap_1_path).get_data().ravel())\n",
    "\n",
    "    # Join all subjects to perform micro-average\n",
    "    y_true = list(chain.from_iterable(original_data))\n",
    "    y_pred = list(chain.from_iterable(predicted_data))\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_pred)\n",
    "    # Optimal threshold is where tpr - fpr is maximum\n",
    "    d = tpr - fpr\n",
    "    idx = np.argwhere(d == np.max(d))\n",
    "    th_op = thresholds[idx]\n",
    "    \n",
    "    all_test_metrics_tht_v1[os.path.basename(model)] = []\n",
    "    # Recompute test metrics with optimal threshold from validation cases\n",
    "    for _, _, subject_label, _, _, pmap_1_path, mask_path, _ in test_results[os.path.basename(model)]:\n",
    "        original_data = nib.load(subject_label).get_data()\n",
    "        predicted_data = nib.load(pmap_1_path).get_data()\n",
    "        \n",
    "        # Compute new metrics after new threshold\n",
    "        metrics = get_metrics(subject_label, pmap_1_path, mask_path, th_op)\n",
    "        metrics['TH_OP'] = th_op\n",
    "        all_test_metrics_tht_v1[os.path.basename(model)].append(metrics)\n",
    "        \n",
    "    metrics = np.array(all_test_metrics_tht_v1[os.path.basename(model)])\n",
    "    test_metrics_tht_v1 = {}\n",
    "    test_metrics_tht_v1['mean'] = {k : np.nanmean([t[k] for t in metrics]) for k in metrics[0]}\n",
    "    test_metrics_tht_v1['std'] = {k : np.nanstd([t[k] for t in metrics]) for k in metrics[0]}\n",
    "\n",
    "    with open(os.path.join(model, 'test_metrics_tht_v1.pkl'), 'wb') as output:\n",
    "        pickle.dump(test_metrics_tht_v1, output, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# Save all metrics after threshold tuining for future reference\n",
    "with open(os.path.join(root, model_name +  '_test_results_tht_v1.pkl'), 'wb') as output:\n",
    "    pickle.dump(all_test_metrics_tht_v1, output, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load each model's metrics after **threshold tuning v1**, compute mean and std."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = []\n",
    "for model in trained_models:\n",
    "    with open(os.path.join(model, 'test_metrics_tht_v1.pkl'), 'rb') as input:\n",
    "        metrics.append(pickle.load(input))\n",
    "\n",
    "metrics = np.array(metrics)\n",
    "test_metrics_tht_v1['mean'] = {k : np.nanmean([t['mean'][k] for t in metrics]) for k in metrics[0]['mean']}\n",
    "test_metrics_tht_v1['std'] = {k : np.nanstd([t['std'][k] for t in metrics]) for k in metrics[0]['std']}\n",
    "\n",
    "# Save final experiment metrics after tht_v1\n",
    "with open(os.path.join(root, model_name + '_test_metrics_tht_v1.pkl'), 'wb') as output:\n",
    "    pickle.dump(test_metrics_tht_v1, output, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FILL HOLES**\n",
    "\n",
    "Apply morphological operation of closing to fill holes in the predicted label (predicted segmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define struct\n",
    "#struct = ndimage.generate_binary_structure(3, 3)\n",
    "struct = ball(3)\n",
    "all_test_metrics_de = {}\n",
    "for model in trained_models:\n",
    "    all_test_metrics_de[os.path.basename(model)] = []\n",
    "    with open(os.path.join(model, 'test_metrics_tht_v0.pkl'), 'rb') as input:\n",
    "        metrics = pickle.load(input)\n",
    "        th = metrics['mean']['TH_OP']\n",
    "        \n",
    "    iterations = []\n",
    "    for _, _, subject_label, pred_label, _, pmap_1_path, mask_path, _ in val_results[os.path.basename(model)]:\n",
    "        original_data = nib.load(subject_label).get_data()\n",
    "        predicted_data = nib.load(pmap_1_path).get_data()\n",
    "        predicted_label = nib.load(pred_label).get_data()\n",
    "        \n",
    "        scores = []\n",
    "        for i in range(1,11):\n",
    "            try:\n",
    "                # Apply morphological operation of closing\n",
    "                #cl_data = binary_closing(predicted_data > th, struct, iterations=i)\n",
    "                cl_data = binary_closing(predicted_label, struct, iterations=i)\n",
    "                # Compute F1S\n",
    "                scores.append(f1_score(original_data.ravel(), cl_data.ravel()))\n",
    "            except:\n",
    "                break\n",
    "        # Get number of iterations that achieved max F1S\n",
    "        idx = np.argwhere(scores == np.max(scores))[0]\n",
    "        iterations.append(idx+1)        \n",
    "        \n",
    "    mean_iter = int(np.floor(np.mean(iterations)))\n",
    "    for _, _, subject_label, pred_label, _, pmap_1_path, mask_path, _ in test_results[os.path.basename(model)]:\n",
    "        original_data = nib.load(subject_label).get_data()\n",
    "        predicted_data = nib.load(pmap_1_path).get_data()\n",
    "        predicted_label = nib.load(pred_label).get_data()\n",
    "        \n",
    "        # Apply morphological operation of closing\n",
    "        #img = binary_closing(predicted_data > th, struct, iterations=mean_iter)\n",
    "        img = binary_closing(predicted_label, struct, iterations=mean_iter)\n",
    "        # Save image temporarly\n",
    "        temp_path = 'temp.nii.gz'\n",
    "        nib.save(nib.Nifti1Image(img.astype(int), nib.load(subject_label).affine), temp_path)\n",
    "        \n",
    "        # Recompute metrics\n",
    "        metrics = get_metrics(subject_label, temp_path, mask_path, 0)\n",
    "        metrics['TH_OP'] = 0\n",
    "        metrics['N_iter'] = mean_iter\n",
    "        all_test_metrics_de[os.path.basename(model)].append(metrics)\n",
    "        \n",
    "    metrics = np.array(all_test_metrics_de[os.path.basename(model)])\n",
    "    test_metrics_de = {}\n",
    "    test_metrics_de['mean'] = {k : np.nanmean([t[k] for t in metrics]) for k in metrics[0]}\n",
    "    test_metrics_de['std'] = {k : np.nanstd([t[k] for t in metrics]) for k in metrics[0]}\n",
    "\n",
    "    with open(os.path.join(model, 'test_metrics_de.pkl'), 'wb') as output:\n",
    "        pickle.dump(test_metrics_de, output, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# Save all metrics after dilation-erosion for future reference\n",
    "with open(os.path.join(root, model_name +  '_all_test_metrics_de.pkl'), 'wb') as output:\n",
    "    pickle.dump(all_test_metrics_de, output, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = []\n",
    "for model in trained_models:\n",
    "    with open(os.path.join(model, 'test_metrics_de.pkl'), 'rb') as input:\n",
    "        metrics.append(pickle.load(input))\n",
    "\n",
    "metrics = np.array(metrics)\n",
    "test_metrics_de['mean'] = {k : np.nanmean([t['mean'][k] for t in metrics]) for k in metrics[0]['mean']}\n",
    "test_metrics_de['std'] = {k : np.nanstd([t['std'][k] for t in metrics]) for k in metrics[0]['std']}\n",
    "\n",
    "# Save final experiment metrics after tht_v1\n",
    "with open(os.path.join(root, model_name + '_test_metrics_de.pkl'), 'wb') as output:\n",
    "    pickle.dump(test_metrics_de, output, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PROCESS TRAINING AND VALIDATION RESULTS**\n",
    "\n",
    "Plot and save training progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in trained_models:\n",
    "    # Plot and save training progress\n",
    "    os.system(\"python ischleseg/deepmedic/plotSaveTrainingProgress.py \" +\n",
    "              os.path.join(model, \"output/logs/trainSession.txt -d -m 20 -s\"))\n",
    "    # Move files to the corresponding model directory\n",
    "    os.system(\"mv trainingProgress.pdf \" + os.path.join(model, 'trainingProgress_' + os.path.basename(model) + '.pdf'))\n",
    "    os.system(\"mv trainingProgress.pkl \" + os.path.join(model, 'trainingProgress.pkl'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load training metrics and compute mean and variance between models (includes training and validation metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load \"measuredMetricsFromAllExperiments\"\n",
    "# 1st dimension: \"Validation\" (0), \"Training\" (1)\n",
    "# 2nd dimension: ? (0)\n",
    "# 3rd dimension: \"Mean Accuracy\" (0), \"Sensitivity\" (1), \"Specificity\" (2), \"DSC (samples)\" (3), \"DSC (full-segm)\" (4)\n",
    "\n",
    "metrics = {}\n",
    "for model in trained_models:\n",
    "    with open(os.path.join(model, 'trainingProgress.pkl'), 'rb') as input:\n",
    "        metrics[os.path.basename(model)] = np.array(pickle.load(input))\n",
    "        metrics[os.path.basename(model)][0,0,4] = np.array(metrics[os.path.basename(model)][0,0,4])\n",
    "        \n",
    "# Compute mean and variance of all models' variations metrics\n",
    "metrics_mean = {}\n",
    "metrics_std = {}\n",
    "metrics_values = np.array(metrics.values())\n",
    "metrics_names_0 = ['Validation', 'Training']\n",
    "metrics_names_1 = ['Mean Accuracy', 'Sensitivity', 'Specificity', 'DSC (Samples)', 'DSC (full-segm)']\n",
    "\n",
    "for i in range(len(metrics_names_0)):\n",
    "    metrics_mean[metrics_names_0[i]] = {}\n",
    "    metrics_std[metrics_names_0[i]] = {}\n",
    "    for j in range(len(metrics_names_1)):\n",
    "        if i == 1 and j == 4: # Skip DSC_full for training (is never calculated)\n",
    "            metrics_mean[metrics_names_0[i]][metrics_names_1[j]] = np.zeros(35*20)\n",
    "            metrics_std[metrics_names_0[i]][metrics_names_1[j]] = np.zeros(35*20)\n",
    "            continue \n",
    "        metrics_mean[metrics_names_0[i]][metrics_names_1[j]] = np.mean(metrics_values[:,i,0,j])\n",
    "        metrics_std[metrics_names_0[i]][metrics_names_1[j]] = np.std(metrics_values[:,i,0,j])\n",
    "\n",
    "train_val_metrics = {}\n",
    "train_val_metrics['mean'] = metrics_mean\n",
    "train_val_metrics['std'] = metrics_std\n",
    "# Save final experiment progress metrics\n",
    "with open(os.path.join(root, model_name + '_train_val_metrics.pkl'), 'wb') as output:\n",
    "    pickle.dump(train_val_metrics, output, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot mean training and validation progress metrics of all trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = []\n",
    "for model in trained_models:\n",
    "    with open(os.path.join(model, 'test_metrics.pkl'), 'rb') as input:\n",
    "        metrics.append(pickle.load(input))\n",
    "\n",
    "plt.close('all')\n",
    "rows, cols = [2, 5]\n",
    "fig = plt.figure(figsize=(cols*6, rows*4))\n",
    "for i in range(len(metrics_names_0)):\n",
    "    for j in range(len(metrics_names_1)):\n",
    "        ax = fig.add_subplot(rows, cols, i * cols + 1 + j)\n",
    "        upper = np.minimum(metrics_mean[metrics_names_0[i]][metrics_names_1[j]] +\n",
    "                           metrics_std[metrics_names_0[i]][metrics_names_1[j]], 1)\n",
    "        lower = np.maximum(metrics_mean[metrics_names_0[i]][metrics_names_1[j]] -\n",
    "                           metrics_std[metrics_names_0[i]][metrics_names_1[j]], 0)\n",
    "        if i == 0 and j == 4:\n",
    "            x = np.arange(0, 40, 5)\n",
    "        else:\n",
    "            x = np.arange(0, 35, 1/20.0)\n",
    "        \n",
    "        plt.plot(x, metrics_mean[metrics_names_0[i]][metrics_names_1[j]], 'r')        \n",
    "        plt.fill_between(x, lower, upper,\n",
    "                         color='grey', alpha=.2, label=r'$\\pm$ 1 std. dev.')\n",
    "        plt.xlim(0, 35)\n",
    "        plt.ylim(0, 1.0)\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel(metrics_names_0[i])\n",
    "        plt.title(metrics_names_1[j])\n",
    "        plt.legend(loc=(0.3, -0.4))\n",
    "        ax.yaxis.grid(True)\n",
    "\n",
    "# Save mean training and validation metrics of all trained models averaged\n",
    "plt.subplots_adjust(hspace=0.7)\n",
    "plt.savefig(os.path.join(root, model_name + '_meanTrainValProgress.pdf'), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# V0 files\n",
    "with open('/home/uziel/DISS/milestones_6/V0_test_metrics.pkl', 'rb') as input:\n",
    "    test_metrics_V0 = pickle.load(input)\n",
    "with open('/home/uziel/DISS/milestones_6/V0_test_metrics_tht_v0.pkl', 'rb') as input:\n",
    "    test_metrics_V0_tht_v0 = pickle.load(input)\n",
    "with open('/home/uziel/DISS/milestones_6/V0_test_metrics_tht_v1.pkl', 'rb') as input:\n",
    "    test_metrics_V0_tht_v1 = pickle.load(input)\n",
    "with open('/home/uziel/DISS/milestones_6/V0_test_metrics_de.pkl', 'rb') as input:\n",
    "    test_metrics_V0_de = pickle.load(input)\n",
    "\n",
    "# V1 files\n",
    "with open('/home/uziel/DISS/milestones_6/V1_test_metrics.pkl', 'rb') as input:\n",
    "    test_metrics_V1 = pickle.load(input)\n",
    "with open('/home/uziel/DISS/milestones_6/V1_test_metrics_tht_v0.pkl', 'rb') as input:\n",
    "    test_metrics_V1_tht_v0 = pickle.load(input)\n",
    "with open('/home/uziel/DISS/milestones_6/V1_test_metrics_tht_v1.pkl', 'rb') as input:\n",
    "    test_metrics_V1_tht_v1 = pickle.load(input)\n",
    "with open('/home/uziel/DISS/milestones_6/V1_test_metrics_de.pkl', 'rb') as input:\n",
    "    test_metrics_V1_de = pickle.load(input)\n",
    "\n",
    "# V2 files\n",
    "with open('/home/uziel/DISS/milestones_6/V2_test_metrics.pkl', 'rb') as input:\n",
    "    test_metrics_V2 = pickle.load(input)\n",
    "with open('/home/uziel/DISS/milestones_6/V2_test_metrics_tht_v0.pkl', 'rb') as input:\n",
    "    test_metrics_V2_tht_v0 = pickle.load(input)\n",
    "with open('/home/uziel/DISS/milestones_6/V2_test_metrics_tht_v1.pkl', 'rb') as input:\n",
    "    test_metrics_V2_tht_v1 = pickle.load(input)\n",
    "with open('/home/uziel/DISS/milestones_6/V2_test_metrics_de.pkl', 'rb') as input:\n",
    "    test_metrics_V2_de = pickle.load(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# V0_R files\n",
    "with open('/home/uziel/DISS/milestones_6/V0_R_test_metrics.pkl', 'rb') as input:\n",
    "    test_metrics_V0_R = pickle.load(input)\n",
    "with open('/home/uziel/DISS/milestones_6/V0_R_test_metrics_tht_v0.pkl', 'rb') as input:\n",
    "    test_metrics_V0_R_tht_v0 = pickle.load(input)\n",
    "with open('/home/uziel/DISS/milestones_6/V0_R_test_metrics_tht_v1.pkl', 'rb') as input:\n",
    "    test_metrics_V0_R_tht_v1 = pickle.load(input)\n",
    "with open('/home/uziel/DISS/milestones_6/V0_R_test_metrics_de.pkl', 'rb') as input:\n",
    "    test_metrics_V0_R_de = pickle.load(input)\n",
    "    \n",
    "# V0_R_transfer files\n",
    "with open('/home/uziel/DISS/milestones_5/V0_R_transfer_test_metrics.pkl', 'rb') as input:\n",
    "    test_metrics_V0_R_transfer = pickle.load(input)\n",
    "with open('/home/uziel/DISS/milestones_5/V0_R_transfer_test_metrics_tht_v0.pkl', 'rb') as input:\n",
    "    test_metrics_V0_R_transfer_tht_v0 = pickle.load(input)\n",
    "with open('/home/uziel/DISS/milestones_5/V0_R_transfer_test_metrics_tht_v1.pkl', 'rb') as input:\n",
    "    test_metrics_V0_R_transfer_tht_v1 = pickle.load(input)\n",
    "with open('/home/uziel/DISS/milestones_5/V0_R_transfer_test_metrics_de.pkl', 'rb') as input:\n",
    "    test_metrics_V0_R_transfer_de = pickle.load(input)\n",
    "    \n",
    "# V1_R files\n",
    "with open('/home/uziel/DISS/milestones_5/V1_R_test_metrics.pkl', 'rb') as input:\n",
    "    test_metrics_V1_R = pickle.load(input)\n",
    "with open('/home/uziel/DISS/milestones_5/V1_R_test_metrics_tht_v0.pkl', 'rb') as input:\n",
    "    test_metrics_V1_R_tht_v0 = pickle.load(input)\n",
    "with open('/home/uziel/DISS/milestones_5/V1_R_test_metrics_tht_v1.pkl', 'rb') as input:\n",
    "    test_metrics_V1_R_tht_v1 = pickle.load(input)\n",
    "with open('/home/uziel/DISS/milestones_5/V1_R_test_metrics_de.pkl', 'rb') as input:\n",
    "    test_metrics_V0_R_de = pickle.load(input)\n",
    "    \n",
    "# V1_R_transfer files\n",
    "with open('/home/uziel/DISS/milestones_5/V1_R_transfer_test_metrics.pkl', 'rb') as input:\n",
    "    test_metrics_V1_R_transfer = pickle.load(input)\n",
    "with open('/home/uziel/DISS/milestones_5/V1_R_transfer_test_metrics_tht_v0.pkl', 'rb') as input:\n",
    "    test_metrics_V1_R_transfer_tht_v0 = pickle.load(input)\n",
    "with open('/home/uziel/DISS/milestones_5/V1_R_transfer_test_metrics_tht_v1.pkl', 'rb') as input:\n",
    "    test_metrics_V1_R_transfer_tht_v1 = pickle.load(input)\n",
    "with open('/home/uziel/DISS/milestones_5/V1_R_transfer_test_metrics_de.pkl', 'rb') as input:\n",
    "    test_metrics_V0_R_transfer_de = pickle.load(input)\n",
    "    \n",
    "# V2_R files\n",
    "with open('/home/uziel/DISS/milestones_5/V2_R_test_metrics.pkl', 'rb') as input:\n",
    "    test_metrics_V2_R = pickle.load(input)\n",
    "with open('/home/uziel/DISS/milestones_5/V2_R_test_metrics_tht_v0.pkl', 'rb') as input:\n",
    "    test_metrics_V2_R_tht_v0 = pickle.load(input)\n",
    "with open('/home/uziel/DISS/milestones_5/V2_R_test_metrics_tht_v1.pkl', 'rb') as input:\n",
    "    test_metrics_V2_R_tht_v1 = pickle.load(input)\n",
    "with open('/home/uziel/DISS/milestones_5/V2_R_test_metrics_de.pkl', 'rb') as input:\n",
    "    test_metrics_V0_R_de = pickle.load(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ERROR BARS** for test cases\n",
    "\n",
    "Plot [0,1] metrics error bars in a 4x3 figure. Each row represents the results of each detection step (0=network output, 1=threshold tuning v0, 2=threshold tuning v1, 3=fill holes). Each column represents the error bars for the three groups of strokes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load subjects per stroke subtypes\n",
    "stroke_types ={'1': ['training_6', 'training_9', 'training_10', 'training_11', 'training_12',\n",
    "                     'training_19', 'training_35', 'training_39', 'training_40', 'training_42'],\n",
    "               '2': ['training_1', 'training_2', 'training_15', 'training_21', 'training_28',\n",
    "                     'training_37', 'training_41'],\n",
    "               '3': ['training_4', 'training_5', 'training_7', 'training_8', 'training_13',\n",
    "                     'training_14', 'training_16', 'training_18', 'training_20', 'training_22',\n",
    "                     'training_23', 'training_24', 'training_26', 'training_27', 'training_30',\n",
    "                     'training_31', 'training_32', 'training_33', 'training_36', 'training_38',\n",
    "                     'training_43', 'training_44', 'training_45', 'training_46', 'training_47',\n",
    "                     'training_48']}\n",
    "\n",
    "# Load all test cases from all folds (43)\n",
    "error_bars_metrics = {'0':{'1':[], '2':[], '3':[]},\n",
    "                      '1':{'1':[], '2':[], '3':[]},\n",
    "                      '2':{'1':[], '2':[], '3':[]},\n",
    "                      '3':{'1':[], '2':[], '3':[]}}\n",
    "# Define keys that will be part of the error bars\n",
    "keys = ['ACC', 'BM', 'F1S', 'JI', 'MCC', 'MK', 'NPV', 'PPV', 'TNR', 'TPR']\n",
    "\n",
    "for model in trained_models:\n",
    "    # Load metrics from network\n",
    "    with open(os.path.join(root, model_name +  '_test_results.pkl'), 'rb') as input:\n",
    "        all_test_metrics = pickle.load(input)\n",
    "    # Load metrics from tht_v0\n",
    "    with open(os.path.join(root, model_name +  '_test_results_tht_v0.pkl'), 'rb') as input:\n",
    "        all_test_metrics_tht_v0 = pickle.load(input)\n",
    "    # Load metrics from tht_v1\n",
    "    with open(os.path.join(root, model_name +  '_test_results_tht_v1.pkl'), 'rb') as input:\n",
    "        all_test_metrics_tht_v1 = pickle.load(input)\n",
    "    # Load metrics from de\n",
    "    with open(os.path.join(root, model_name +  '_all_test_metrics_de.pkl'), 'rb') as input:\n",
    "        all_test_metrics_de = pickle.load(input)\n",
    "\n",
    "    for i in range(len(all_test_metrics[os.path.basename(model)])):\n",
    "        \n",
    "        subject, _, _, _, _, _, _, metrics = all_test_metrics[os.path.basename(model)][i]\n",
    "        stroke_type = [key for key in stroke_types.keys() if subject in stroke_types[key]][0]\n",
    "        # Store only the metrics that go from 0 to 1\n",
    "        error_bars_metrics['0'][stroke_type].append([metrics[key] for key in keys])        \n",
    "\n",
    "        metrics = all_test_metrics_tht_v0[os.path.basename(model)][i]\n",
    "        error_bars_metrics['1'][stroke_type].append([metrics[key] for key in keys])        \n",
    "        \n",
    "        metrics = all_test_metrics_tht_v1[os.path.basename(model)][i]\n",
    "        error_bars_metrics['2'][stroke_type].append([metrics[key] for key in keys])\n",
    "        \n",
    "        metrics = all_test_metrics_de[os.path.basename(model)][i]\n",
    "        error_bars_metrics['3'][stroke_type].append([metrics[key] for key in keys])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_labels = ['Base', 'TH Tuning V0', 'TH Tuning V1', 'Fill Holes']\n",
    "col_labels = ['Lacunar/Subcortical', 'Small cortical', 'Big cortical/Main artery']\n",
    "\n",
    "plt.close('all')\n",
    "rows, cols = [4, 3]\n",
    "fig, axes = plt.subplots(nrows=rows, ncols=cols, figsize=(15, 10))\n",
    "\n",
    "for ax, col in zip(axes[0], col_labels):\n",
    "    ax.set_title(col)\n",
    "\n",
    "for ax, row in zip(axes[:,0], row_labels):\n",
    "    ax.set_ylabel(row, rotation='vertical', size='large')\n",
    "\n",
    "for i in range(rows):\n",
    "    for j in range(1,cols+1):\n",
    "        # Compute mean and confidence interval\n",
    "        metrics = error_bars_metrics[str(i)][str(j)]\n",
    "        mean = np.mean(metrics, axis=0)\n",
    "        confidence_interval = st.t.interval(0.95, len(metrics[0])-1,\n",
    "                                            loc=mean, scale=st.sem(metrics, axis=0))\n",
    "        differences = (abs(confidence_interval[0] - mean), abs(confidence_interval[1] - mean))\n",
    "        \n",
    "        # Plot error bar\n",
    "        axes[i,j-1].grid()\n",
    "        axes[i,j-1].set_ylim([0,1])\n",
    "        axes[i,j-1].tick_params(axis='x', width=10)\n",
    "        (_, caps, _) = axes[i,j-1].errorbar(keys, mean, differences,\n",
    "                                    color='red', mew=5,\n",
    "                                    fmt='o', markersize=3, capsize=10)\n",
    "        for cap in caps:\n",
    "            cap.set_markeredgewidth(1)\n",
    "\n",
    "plt.subplots_adjust(hspace=0.2)\n",
    "plt.savefig(os.path.join(root, model_name + '_test_error_bars.pdf'), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**VISUAL SEGMENTATION COMPARISON**\n",
    "\n",
    "Plot the predicted segmentations for each phase. Rows are subjects, columns phases. Plot three times with the best 5 results of each set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load subjects per stroke subtypes\n",
    "stroke_types ={'1': ['training_6', 'training_9', 'training_10', 'training_11', 'training_12',\n",
    "                     'training_19', 'training_35', 'training_39', 'training_40', 'training_42'],\n",
    "               '2': ['training_1', 'training_2', 'training_15', 'training_21', 'training_28',\n",
    "                     'training_37', 'training_41'],\n",
    "               '3': ['training_4', 'training_5', 'training_7', 'training_8', 'training_13',\n",
    "                     'training_14', 'training_16', 'training_18', 'training_20', 'training_22',\n",
    "                     'training_23', 'training_24', 'training_26', 'training_27', 'training_30',\n",
    "                     'training_31', 'training_32', 'training_33', 'training_36', 'training_38',\n",
    "                     'training_43', 'training_44', 'training_45', 'training_46', 'training_47',\n",
    "                     'training_48']}\n",
    "\n",
    "# Set best cut in z coordinate for each subject\n",
    "tr_cut_coords = {'training_1': [11], 'training_2': [16], 'training_4': [13], 'training_5': [11], 'training_6': [16],\n",
    "                 'training_7': [15], 'training_8': [15], 'training_9': [14], 'training_10': [12], 'training_11': [9],\n",
    "                 'training_12': [12], 'training_13': [15], 'training_14': [15], 'training_15': [16], 'training_16': [16],\n",
    "                 'training_18': [9], 'training_19': [15], 'training_20': [15], 'training_21': [10], 'training_22': [13],\n",
    "                 'training_23': [15], 'training_24': [12], 'training_26': [14], 'training_27': [11], 'training_28': [21],\n",
    "                 'training_30': [14], 'training_31': [19], 'training_32': [14], 'training_33': [18], 'training_35': [17],\n",
    "                 'training_36': [16], 'training_37': [15], 'training_38': [15], 'training_39': [10], 'training_40': [11],\n",
    "                 'training_41': [11], 'training_42': [12], 'training_43': [8], 'training_44': [14], 'training_45': [9],\n",
    "                 'training_46': [12], 'training_47': [15], 'training_48': [12]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all test cases from all folds (43)\n",
    "base_metrics = {'1':[], '2':[], '3':[]}\n",
    "subjects_info = {'1':[], '2':[], '3':[]}\n",
    "\n",
    "# Define keys that will be part of the error bars\n",
    "keys = ['F1S']\n",
    "\n",
    "for model in trained_models:\n",
    "    # Load metrics from network\n",
    "    with open(os.path.join(root, model_name +  '_test_results.pkl'), 'rb') as input:\n",
    "        all_test_metrics = pickle.load(input)\n",
    "    # Load metrics from tht_v0\n",
    "    with open(os.path.join(root, model_name +  '_test_results_tht_v0.pkl'), 'rb') as input:\n",
    "        all_test_metrics_tht_v0 = pickle.load(input)\n",
    "    # Load metrics from tht_v1\n",
    "    with open(os.path.join(root, model_name +  '_test_results_tht_v1.pkl'), 'rb') as input:\n",
    "        all_test_metrics_tht_v1 = pickle.load(input)\n",
    "    # Load metrics from de\n",
    "    with open(os.path.join(root, model_name +  '_all_test_metrics_de.pkl'), 'rb') as input:\n",
    "        all_test_metrics_de = pickle.load(input)\n",
    "\n",
    "    for i in range(len(all_test_metrics[os.path.basename(model)])):\n",
    "        subject, subject_channels, subject_label, pred_path, _, pmap_1_path, _, metrics = all_test_metrics[os.path.basename(model)][i]\n",
    "        stroke_type = [key for key in stroke_types.keys() if subject in stroke_types[key]][0]\n",
    "        subjects_info[stroke_type].append([subject, subject_channels, subject_label, pred_path, pmap_1_path])\n",
    "        \n",
    "        f1s_0 = metrics['F1S']\n",
    "        f1s_1 = all_test_metrics_tht_v0[os.path.basename(model)][i]['F1S']\n",
    "        f1s_2 = all_test_metrics_tht_v1[os.path.basename(model)][i]['F1S']\n",
    "        f1s_3 = all_test_metrics_de[os.path.basename(model)][i]['F1S']\n",
    "        th_op_0 = all_test_metrics_tht_v0[os.path.basename(model)][i]['TH_OP']\n",
    "        th_op_1 = all_test_metrics_tht_v1[os.path.basename(model)][i]['TH_OP']\n",
    "        n_iter = all_test_metrics_de[os.path.basename(model)][i]['N_iter']\n",
    "        \n",
    "        base_metrics[stroke_type].append([f1s_0, f1s_1, f1s_2, f1s_3,\n",
    "                                          th_op_0, th_op_1, n_iter])\n",
    "        \n",
    "# Create all three plots\n",
    "col_labels = ['ADC', 'Ground truth', 'Base', 'TH Tuning V0', 'TH Tuning V1', 'Fill Holes']\n",
    "\n",
    "# generate the colors for your colormap\n",
    "color1 = colorConverter.to_rgba('white')\n",
    "\n",
    "# make the colormaps\n",
    "cmap1 = mpl.colors.LinearSegmentedColormap.from_list('my_cmap',['black','red'],256)\n",
    "\n",
    "cmap1._init() # create the _lut array, with rgba values\n",
    "\n",
    "# create your alpha array and fill the colormap with them.\n",
    "# here it is progressive, but you can create whathever you want\n",
    "alphas = np.linspace(0, 0.8, cmap1.N+3)\n",
    "cmap1._lut[:,-1] = alphas\n",
    "\n",
    "for i in range(1,4):\n",
    "    plt.close('all')\n",
    "    rows, cols = [5,6]\n",
    "    fig, axes = plt.subplots(nrows=rows, ncols=cols, figsize=(15, 10))\n",
    "    \n",
    "    # Get the 5 subjects with the highest F1S\n",
    "    f1s_list = [x[0] for x in base_metrics[str(i)]]\n",
    "    indexes = np.lexsort((range(len(subjects_info[str(i)])), f1s_list))[::-1]\n",
    "    all_subjects = [subjects_info[str(i)][idx] for idx in indexes][:5]    \n",
    "    row_labels = [x[0] for x in all_subjects]\n",
    "    base_metrics[str(i)] = [base_metrics[str(i)][idx] for idx in indexes][:5]\n",
    "    \n",
    "    for ax, col in zip(axes[0], col_labels):\n",
    "        ax.set_title(col, color='w', pad=30)\n",
    "\n",
    "    for ax, row in zip(axes[:,0], row_labels):\n",
    "        ax.set_ylabel(row.split('_')[-1], rotation=0, size='large', color='white')\n",
    "    \n",
    "    \n",
    "    for j in range(len(all_subjects)):\n",
    "        # Cut coords for this subject\n",
    "        cut_coords = tr_cut_coords[all_subjects[j][0]][0]\n",
    "        channel_ADC = all_subjects[j][1][0]\n",
    "        ADC_data = nib.load(channel_ADC).get_data()\n",
    "        \n",
    "        # Find best bounding box\n",
    "        data_2d = ADC_data[:,:,cut_coords]\n",
    "        idx = np.nonzero(data_2d)\n",
    "        # row_min, row_max, col_min, col_max\n",
    "        bbox = [np.min(idx[0]), np.max(idx[0]), np.min(idx[1]), np.max(idx[1])] \n",
    "        \n",
    "        # 0 - ADC Channel\n",
    "        axes[j,0].imshow(ADC_data[bbox[0]:bbox[1],bbox[2]:bbox[3],cut_coords].T, 'gray', interpolation='none')\n",
    "        axes[j,0].set_xlabel('z=%d' % cut_coords, color='w')\n",
    "        axes[j,0].xaxis.set_label_coords(0.5, -0.1)\n",
    "        \n",
    "        # 1 - Ground truth\n",
    "        label_data = nib.load(all_subjects[j][2]).get_data()\n",
    "        axes[j,1].imshow(ADC_data[bbox[0]:bbox[1],bbox[2]:bbox[3],cut_coords].T, 'gray', interpolation='none')\n",
    "        axes[j,1].imshow(label_data[bbox[0]:bbox[1],bbox[2]:bbox[3],cut_coords].T, cmap=cmap1, alpha=0.7, interpolation='none')\n",
    "        axes[j,1].set_xlabel('DSC: 1', color='w')\n",
    "        axes[j,1].xaxis.set_label_coords(0.5, -0.1)\n",
    "        \n",
    "        # 2 - Base prediction\n",
    "        prediction_data = nib.load(all_subjects[j][3]).get_data()\n",
    "        axes[j,2].imshow(ADC_data[bbox[0]:bbox[1],bbox[2]:bbox[3],cut_coords].T, 'gray', interpolation='none')\n",
    "        axes[j,2].imshow(prediction_data[bbox[0]:bbox[1],bbox[2]:bbox[3],cut_coords].T, cmap=cmap1, alpha=0.5, interpolation='none')\n",
    "        axes[j,2].set_xlabel('DSC: %.2f' % base_metrics[str(i)][j][0], color='w')\n",
    "        axes[j,2].xaxis.set_label_coords(0.5, -0.1)\n",
    "        \n",
    "        # 3 - THT V0\n",
    "        thtv0_data = (nib.load(all_subjects[j][4]).get_data() > base_metrics[str(i)][j][4]).astype(int)\n",
    "        axes[j,3].imshow(ADC_data[bbox[0]:bbox[1],bbox[2]:bbox[3],cut_coords].T, 'gray', interpolation='none')\n",
    "        axes[j,3].imshow(thtv0_data[bbox[0]:bbox[1],bbox[2]:bbox[3],cut_coords].T, cmap=cmap1, alpha=0.5, interpolation='none')\n",
    "        axes[j,3].set_xlabel('DSC: %.2f' % base_metrics[str(i)][j][1], color='w')\n",
    "        axes[j,3].xaxis.set_label_coords(0.5, -0.1)\n",
    "        \n",
    "        # 4 - THT V1\n",
    "        thtv1_data = (nib.load(all_subjects[j][4]).get_data() > base_metrics[str(i)][j][5]).astype(int)\n",
    "        axes[j,4].imshow(ADC_data[bbox[0]:bbox[1],bbox[2]:bbox[3],cut_coords].T, 'gray', interpolation='none')\n",
    "        axes[j,4].imshow(thtv0_data[bbox[0]:bbox[1],bbox[2]:bbox[3],cut_coords].T, cmap=cmap1, alpha=0.5, interpolation='none')\n",
    "        axes[j,4].set_xlabel('DSC: %.2f' % base_metrics[str(i)][j][2], color='w')\n",
    "        axes[j,4].xaxis.set_label_coords(0.5, -0.1)\n",
    "        \n",
    "        # 5 - Fill holes\n",
    "        struct = ball(3)\n",
    "        fh_data = binary_closing(nib.load(all_subjects[j][3]).get_data(),\n",
    "                                 struct, iterations=base_metrics[str(i)][j][6]).astype(int)\n",
    "        axes[j,5].imshow(ADC_data[bbox[0]:bbox[1],bbox[2]:bbox[3],cut_coords].T, 'gray', interpolation='none')\n",
    "        axes[j,5].imshow(fh_data[bbox[0]:bbox[1],bbox[2]:bbox[3],cut_coords].T, cmap=cmap1, alpha=0.5, interpolation='none')\n",
    "        axes[j,5].set_xlabel('DSC: %.2f' % base_metrics[str(i)][j][3], color='w')\n",
    "        axes[j,5].xaxis.set_label_coords(0.5, -0.1)\n",
    "\n",
    "    \n",
    "        fig.subplots_adjust(hspace=0.5, wspace=-0.7)\n",
    "\n",
    "    \n",
    "    # save figure\n",
    "    fig.patch.set_facecolor('xkcd:black')\n",
    "    fig.savefig(os.path.join(root, model_name + '_test_seg_comparison_group_%d.pdf' % i),\n",
    "                bbox_inches='tight', facecolor=fig.get_facecolor())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BLAND-ALTMAN PLOT**\n",
    "\n",
    "Plot average DSC and average VOL with respect to volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**METRICS SUMMARY TABLE**\n",
    "\n",
    "Table with as many rows as metrics. Columns: Base, THT V0, % Improvement, THT V1, % Improvement, Fill Holes, % Improvement. Use pandas for the table (dataframe) so that it can be exported to latex: pd.to_latex()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
